{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.loss import CrossEntropyLoss, MSELoss\n",
    "import torch.functional as F\n",
    "from torch.optim import SGD\n",
    "from torchvision import datasets\n",
    "import my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN, N_TEST = 50000, 10000\n",
    "# reduce dimension via a random projection\n",
    "# D = 10\n",
    "# train_data, train_labels, test_data, test_labels = my.unbalanced_mnist(N_TRAIN, N_TEST, D=D)\n",
    "# reduce dimension via PCA\n",
    "train_data, train_labels, test_data, test_labels = my.unbalanced_mnist(N_TRAIN, N_TEST, pca=True)\n",
    "D = train_data.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(classifier, X):\n",
    "    return th.max(classifier(X), 1)[1]\n",
    "\n",
    "def accuracy(y_bar, y):\n",
    "    return th.sum(((y_bar - y) == 0).float()) / float(y.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 1000]cross-entropy loss: 0.468184, accuracy: 0.864980\n",
      "[iteration 2000]cross-entropy loss: 0.315142, accuracy: 0.966840\n",
      "[iteration 3000]cross-entropy loss: 0.241794, accuracy: 0.978700\n",
      "[iteration 4000]cross-entropy loss: 0.200385, accuracy: 0.980720\n",
      "[iteration 5000]cross-entropy loss: 0.174001, accuracy: 0.981340\n",
      "[iteration 6000]cross-entropy loss: 0.155707, accuracy: 0.981600\n",
      "[iteration 7000]cross-entropy loss: 0.142242, accuracy: 0.981780\n",
      "[iteration 8000]cross-entropy loss: 0.131885, accuracy: 0.981900\n",
      "[iteration 9000]cross-entropy loss: 0.123646, accuracy: 0.982220\n",
      "[iteration 10000]cross-entropy loss: 0.116921, accuracy: 0.982420\n"
     ]
    }
   ],
   "source": [
    "# a c optimized by cross-entropy loss\n",
    "c = nn.Linear(D, 2)\n",
    "optim = SGD(c.parameters(), lr=0.001)\n",
    "N_ITERATIONS = 10000\n",
    "for i in range(N_ITERATIONS):\n",
    "    loss = CrossEntropyLoss(size_average=True)(c(train_data), train_labels)\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        a = accuracy(predict(c, train_data), train_labels)\n",
    "        print('[iteration %d]cross-entropy loss: %f, accuracy: %f' % ((i + 1), float(loss[0]), float(a[0])))\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp(y_bar, y): # true positive\n",
    "    return th.sum((y_bar * y).float())\n",
    "\n",
    "def fp(y_bar, y): # false positive\n",
    "    return th.sum((y_bar * (1 - y)).float())\n",
    "\n",
    "def fn(y_bar, y): # false negative\n",
    "    return th.sum(((1 - y_bar) * y).float())\n",
    "\n",
    "def precision(y_bar, y):\n",
    "    tp_, fp_ = tp(y_bar, y), fp(y_bar, y)\n",
    "    # TODO\n",
    "    return tp_ / (tp_ + fp_ + 1)\n",
    "\n",
    "def recall(y_bar, y):\n",
    "    tp_, fn_ = tp(y_bar, y), fn(y_bar, y)\n",
    "    return tp_ / (tp_ + fn_ + 1)\n",
    "\n",
    "def f_beta(y_bar, y, beta=1):\n",
    "    p, r = precision(y_bar, y), recall(y_bar, y)\n",
    "    return (1 + beta ** 2) * p * r / (beta ** 2 * p + r + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.983500, precision: 0.951219, recall: 0.874618, f1: 0.588819\n"
     ]
    }
   ],
   "source": [
    "# baseline performance measures\n",
    "y_bar = predict(classifier, test_data)\n",
    "accuracy_ = accuracy(y_bar, test_labels)\n",
    "precision_ = precision(y_bar, test_labels)\n",
    "recall_ = recall(y_bar, test_labels)\n",
    "f1 = f_beta(y_bar, test_labels)\n",
    "print('accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy_, precision_, recall_, f1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 64\n",
    "STD = 1\n",
    "\n",
    "def sample(X, y):\n",
    "    X, y = X.data.numpy(), y.data.numpy()\n",
    "    idx = np.random.randint(0, len(X) - 1, SAMPLE_SIZE)\n",
    "    X, y = Variable(th.from_numpy(X[idx])), Variable(th.from_numpy(y[idx]))\n",
    "    return X, y\n",
    "\n",
    "def data(classifier, X):\n",
    "    y = classifier(X)\n",
    "    Xy = th.cat((X, y), 1)\n",
    "    Xy = Xy.view(1, Xy.numel())\n",
    "    W, b = classifier.weight, classifier.bias\n",
    "    W, b = W.view(1, W.numel()), b.view(1, b.numel())\n",
    "    return th.cat((Xy, W, b), 1)\n",
    "\n",
    "def L(classifier, X, y):\n",
    "    y_bar = predict(classifier, X)\n",
    "    f1 = f_beta(y_bar, y)\n",
    "    return f1\n",
    "\n",
    "def perturb(classifier):\n",
    "    perturbed = deepcopy(classifier)\n",
    "    perturbed.weight.data += th.randn(perturbed.weight.data.size()) * STD\n",
    "    perturbed.bias.data += th.randn(perturbed.bias.data.size()) * STD\n",
    "    return perturbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "Let $c$ be a classifier and $D=\\{(X_1, y_1),...,(X_N, y_N)\\}$ be the set of training data. In order to minimize $L(c, D)$, where $L$ is a non-decomposable loss function, we introduce $L_\\theta$, a parameterized approximation of $L(c, D)$, and update $c$ as follows:\n",
    "\n",
    "1. Compute $\\delta = L(c, D)-L(\\bar{c},D)$, where $\\bar{c}$ is obtained by stochastically perturbing the parameters of $c$\n",
    "\n",
    "2. Randomly sample $K$ subsets, $D_1, ..., D_K$, of $D$ (these subsets may vary in cardinality)\n",
    "\n",
    "3. Minimize $(\\delta - \\frac1K \\sum_{i = 1}^K \\delta_i)^2$ with respect to $\\theta$, where $\\delta_i = L_\\theta(c, D_i) - L_\\theta(\\bar{c}, D_i)$\n",
    "\n",
    "4. Repeat 1, 2, and 3 several times until $L_\\theta$ becomes a satisfactory approximation of $L$ near $c$\n",
    "\n",
    "5. Randomly sample $K'$ subsets, $D_1, ..., D_K'$, of $D$ and let $c \\leftarrow c - \\alpha \\sum_{i = 1}^K \\frac{\\partial L_\\theta}{\\partial c} (c, D_i)$, where $\\alpha$ is a positive learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nn.Linear(D, 2) # the classifier\n",
    "approx = nn.Sequential(\n",
    "    nn.Linear((D + 2) * SAMPLE_SIZE + D * 2 + 2, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 1)\n",
    ") # L_\\theta\n",
    "c_optim = SGD(c.parameters(), 0.01)\n",
    "approx_optim = SGD(approx.parameters(), 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 1000]objective: 0.013376, f1: 0.153235\n",
      "[iteration 2000]objective: 0.021679, f1: 0.176113\n",
      "[iteration 3000]objective: 0.046296, f1: 0.196153\n",
      "[iteration 4000]objective: 0.025103, f1: 0.213899\n",
      "[iteration 5000]objective: 0.026911, f1: 0.226312\n",
      "[iteration 6000]objective: 0.026471, f1: 0.243124\n",
      "[iteration 7000]objective: 0.036381, f1: 0.261363\n",
      "[iteration 8000]objective: 0.019537, f1: 0.277310\n",
      "[iteration 9000]objective: 0.058926, f1: 0.295101\n",
      "[iteration 10000]objective: 0.065837, f1: 0.313041\n",
      "[iteration 11000]objective: 0.040371, f1: 0.331998\n",
      "[iteration 12000]objective: 0.083660, f1: 0.354511\n",
      "[iteration 13000]objective: 0.076736, f1: 0.374691\n",
      "[iteration 14000]objective: 0.074458, f1: 0.395334\n",
      "[iteration 15000]objective: 0.107419, f1: 0.409446\n",
      "[iteration 16000]objective: 0.075362, f1: 0.426654\n",
      "[iteration 17000]objective: 0.026898, f1: 0.443105\n",
      "[iteration 18000]objective: 0.088195, f1: 0.459822\n",
      "[iteration 19000]objective: 0.043067, f1: 0.477762\n",
      "[iteration 20000]objective: 0.094444, f1: 0.498792\n",
      "[iteration 21000]objective: 0.069526, f1: 0.515473\n",
      "[iteration 22000]objective: 0.050000, f1: 0.528517\n",
      "[iteration 23000]objective: 0.081946, f1: 0.537016\n",
      "[iteration 24000]objective: 0.021308, f1: 0.545239\n",
      "[iteration 25000]objective: 0.048976, f1: 0.552974\n",
      "[iteration 26000]objective: 0.072563, f1: 0.561014\n",
      "[iteration 27000]objective: 0.026724, f1: 0.565724\n",
      "[iteration 28000]objective: 0.023825, f1: 0.567486\n",
      "[iteration 29000]objective: 0.041312, f1: 0.569538\n",
      "[iteration 30000]objective: -0.060524, f1: 0.574174\n",
      "[iteration 31000]objective: -0.045613, f1: 0.576555\n",
      "[iteration 32000]objective: -0.060995, f1: 0.578018\n",
      "[iteration 33000]objective: 0.009709, f1: 0.578952\n",
      "[iteration 34000]objective: -0.004903, f1: 0.581526\n",
      "[iteration 35000]objective: 0.032687, f1: 0.584427\n",
      "[iteration 36000]objective: 0.013536, f1: 0.585596\n",
      "[iteration 37000]objective: -0.073581, f1: 0.586593\n",
      "[iteration 38000]objective: 0.017000, f1: 0.586419\n",
      "[iteration 39000]objective: -0.028344, f1: 0.587433\n",
      "[iteration 40000]objective: -0.034442, f1: 0.589741\n",
      "[iteration 41000]objective: 0.000839, f1: 0.590543\n",
      "[iteration 42000]objective: -0.079623, f1: 0.590061\n",
      "[iteration 43000]objective: 0.019641, f1: 0.590786\n",
      "[iteration 44000]objective: -0.020659, f1: 0.590707\n",
      "[iteration 45000]objective: -0.065470, f1: 0.591920\n",
      "[iteration 46000]objective: -0.060148, f1: 0.592250\n",
      "[iteration 47000]objective: -0.050938, f1: 0.593064\n",
      "[iteration 48000]objective: -0.030594, f1: 0.593064\n",
      "[iteration 49000]objective: -0.057046, f1: 0.593397\n",
      "[iteration 50000]objective: -0.018505, f1: 0.593807\n"
     ]
    }
   ],
   "source": [
    "OUTER = 50000\n",
    "INNER = 10\n",
    "K = 10\n",
    "\n",
    "for i in range(OUTER):\n",
    "    total_mse = 0\n",
    "    total_delta, total_delta_ = 0, 0\n",
    "    for j in range(INNER):\n",
    "        c_bar = perturb(c)\n",
    "        delta = L(c, train_data, train_labels) - L(c_bar, train_data, train_labels) # \\delta = L(c, D)-L(\\bar{c},D)\n",
    "\n",
    "        samples = [sample(train_data, train_labels) for _ in range(K)] # D_1, ..., D_K\n",
    "        c_d = th.cat(map(lambda X: data(c, X), zip(*samples)[0]), 0) # (c, D_1), ..., (c, D_K)\n",
    "        c_bar_d = th.cat(map(lambda X: data(c_bar, X), zip(*samples)[0]), 0) # (c_bar, D_1), ..., (c_bar, D_K)\n",
    "        # \\frac1K \\sum_{i = 1}^K \\delta_i, where \\delta_i = L_\\theta(c, D_i) - L_\\theta(\\bar{c}, D_i)\n",
    "        delta_ = th.mean(approx(c_d) - approx(c_bar_d), 0)\n",
    "        \n",
    "        total_delta += abs(float(delta))\n",
    "        total_delta_ += abs(float(delta_))\n",
    "\n",
    "        # \\arg \\min_\\theta (\\delta - \\frac1K \\sum_{i = 1}^K \\delta_i)^2\n",
    "        mse = MSELoss()(delta_, delta)\n",
    "        approx_optim.zero_grad()\n",
    "        mse.backward()\n",
    "        approx_optim.step()\n",
    "        total_mse += float(mse)\n",
    "    \n",
    "#     if (i + 1) % 100 == 0:\n",
    "#         print('[iteration %d]mse: %f, delta: %f, delta_: %f' % (\n",
    "#             (i + 1), total_mse / (j + 1), total_delta / (j + 1), total_delta_ / (j + 1)))\n",
    "        \n",
    "    samples = [sample(train_data, train_labels) for _ in range(K)] # D_1, ..., D_K\n",
    "    c_d = th.cat(map(lambda X: data(c, X), zip(*samples)[0]), 0) # (c, D_1), ..., (c, D_K)\n",
    "    # \\arg \\min_c \\frac1K \\sum_{i = 1}^K L_\\theta (c, D_i)\n",
    "    objective = -th.mean(approx(c_d))\n",
    "    c_optim.zero_grad()\n",
    "    objective.backward()\n",
    "    c_optim.step()\n",
    "    \n",
    "    if (i + 1) % 1000 == 0:\n",
    "        y_bar = predict(c, test_data)\n",
    "        f1 = f_beta(y_bar, test_labels)\n",
    "        print('[iteration %d]objective: %f, f1: %f' % ((i + 1), float(objective), float(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.984000, precision: 0.917431, recall: 0.917431, f1: 0.593807\n"
     ]
    }
   ],
   "source": [
    "y_bar = predict(c, test_data)\n",
    "accuracy_ = accuracy(y_bar, test_labels)\n",
    "precision_ = precision(y_bar, test_labels)\n",
    "recall_ = recall(y_bar, test_labels)\n",
    "f1 = f_beta(y_bar, test_labels)\n",
    "print('accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy_, precision_, recall_, f1))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
