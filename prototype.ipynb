{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.loss import CrossEntropyLoss, MSELoss\n",
    "import torch.functional as F\n",
    "from torch.optim import SGD\n",
    "from torchvision import datasets\n",
    "import my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN, N_TEST = 5000, 1000\n",
    "# reduce dimension via a random projection\n",
    "# D = 10\n",
    "# train_data, train_labels, test_data, test_labels = my.unbalanced_mnist(N_TRAIN, N_TEST, D=D)\n",
    "# reduce dimension via PCA\n",
    "train_data, train_labels, test_data, test_labels = my.unbalanced_mnist(N_TRAIN, N_TEST, pca=True)\n",
    "D = train_data.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(classifier, X):\n",
    "    return th.max(classifier(X), 1)[1]\n",
    "\n",
    "def accuracy(y_bar, y):\n",
    "    return th.sum(((y_bar - y) == 0).float()) / float(y.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a c optimized by cross-entropy loss\n",
    "c = nn.Linear(D, 2)\n",
    "optim = SGD(c.parameters(), lr=0.001)\n",
    "N_ITERATIONS = 10000\n",
    "for i in range(N_ITERATIONS):\n",
    "    loss = CrossEntropyLoss(size_average=True)(c(train_data), train_labels)\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        a = accuracy(predict(c, train_data), train_labels)\n",
    "        print('[iteration %d]cross-entropy loss: %f, accuracy: %f' % ((i + 1), float(loss[0]), float(a[0])))\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp(y_bar, y): # true positive\n",
    "    return th.sum((y_bar * y).float())\n",
    "\n",
    "def fp(y_bar, y): # false positive\n",
    "    return th.sum((y_bar * (1 - y)).float())\n",
    "\n",
    "def fn(y_bar, y): # false negative\n",
    "    return th.sum(((1 - y_bar) * y).float())\n",
    "\n",
    "def precision(y_bar, y):\n",
    "    tp_, fp_ = tp(y_bar, y), fp(y_bar, y)\n",
    "    # TODO\n",
    "    return tp_ / (tp_ + fp_ + 1e-5)\n",
    "\n",
    "def recall(y_bar, y):\n",
    "    tp_, fn_ = tp(y_bar, y), fn(y_bar, y)\n",
    "    return tp_ / (tp_ + fn_ + 1e-5)\n",
    "\n",
    "def f_beta(y_bar, y, beta=1):\n",
    "    p, r = precision(y_bar, y), recall(y_bar, y)\n",
    "    return (1 + beta ** 2) * p * r / (beta ** 2 * p + r + 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline performance measures\n",
    "y_bar = predict(c, test_data)\n",
    "accuracy_ = accuracy(y_bar, test_labels)\n",
    "precision_ = precision(y_bar, test_labels)\n",
    "recall_ = recall(y_bar, test_labels)\n",
    "f1 = f_beta(y_bar, test_labels)\n",
    "print('accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy_, precision_, recall_, f1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 64\n",
    "STD = 1\n",
    "\n",
    "def sample(X, y):\n",
    "    X, y = X.data.numpy(), y.data.numpy()\n",
    "    idx = np.random.randint(0, len(X) - 1, SAMPLE_SIZE)\n",
    "    X, y = Variable(th.from_numpy(X[idx])), Variable(th.from_numpy(y[idx]))\n",
    "    return X, y\n",
    "\n",
    "def data(classifier, X):\n",
    "    y = classifier(X)\n",
    "    Xy = th.cat((X, y), 1)\n",
    "    Xy = Xy.view(1, Xy.numel())\n",
    "    W, b = classifier.weight, classifier.bias\n",
    "    W, b = W.view(1, W.numel()), b.view(1, b.numel())\n",
    "    return th.cat((Xy, W, b), 1)\n",
    "\n",
    "def L(classifier, X, y):\n",
    "    y_bar = predict(classifier, X)\n",
    "    f1 = f_beta(y_bar, y)\n",
    "    return f1\n",
    "\n",
    "def perturb(classifier):\n",
    "    perturbed = deepcopy(classifier)\n",
    "    perturbed.weight.data += th.randn(perturbed.weight.data.size()) * STD\n",
    "    perturbed.bias.data += th.randn(perturbed.bias.data.size()) * STD\n",
    "    return perturbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "Let $c$ be a classifier and $D=\\{(X_1, y_1),...,(X_N, y_N)\\}$ be the set of training data. In order to minimize $L(c, D)$, where $L$ is a non-decomposable loss function, we introduce $L_\\theta$, a parameterized approximation of $L(c, D)$, and update $c$ as follows:\n",
    "\n",
    "1. Compute $\\delta = L(c, D)-L(\\bar{c},D)$, where $\\bar{c}$ is obtained by stochastically perturbing the parameters of $c$\n",
    "\n",
    "2. Randomly sample $K$ subsets, $D_1, ..., D_K$, of $D$ (these subsets may vary in cardinality)\n",
    "\n",
    "3. Minimize $(\\delta - \\frac1K \\sum_{i = 1}^K \\delta_i)^2$ with respect to $\\theta$, where $\\delta_i = L_\\theta(c, D_i) - L_\\theta(\\bar{c}, D_i)$\n",
    "\n",
    "4. Repeat 1, 2, and 3 several times until $L_\\theta$ becomes a satisfactory approximation of $L$ near $c$\n",
    "\n",
    "5. Randomly sample $K'$ subsets, $D_1, ..., D_K'$, of $D$ and let $c \\leftarrow c - \\alpha \\sum_{i = 1}^K \\frac{\\partial L_\\theta}{\\partial c} (c, D_i)$, where $\\alpha$ is a positive learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nn.Linear(D, 2) # the classifier\n",
    "approx = nn.Sequential(\n",
    "    nn.Linear((D + 2) * SAMPLE_SIZE + D * 2 + 2, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 1)\n",
    ") # L_\\theta\n",
    "c_optim = SGD(c.parameters(), 0.01)\n",
    "approx_optim = SGD(approx.parameters(), 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTER = 50000\n",
    "INNER = 10\n",
    "K = 10\n",
    "\n",
    "for i in range(OUTER):\n",
    "    total_mse = 0\n",
    "    total_delta, total_delta_ = 0, 0\n",
    "    for j in range(INNER):\n",
    "        c_bar = perturb(c)\n",
    "        delta = L(c, train_data, train_labels) - L(c_bar, train_data, train_labels) # \\delta = L(c, D)-L(\\bar{c},D)\n",
    "\n",
    "        samples = [sample(train_data, train_labels) for _ in range(K)] # D_1, ..., D_K\n",
    "        c_d = th.cat(map(lambda X: data(c, X), zip(*samples)[0]), 0) # (c, D_1), ..., (c, D_K)\n",
    "        c_bar_d = th.cat(map(lambda X: data(c_bar, X), zip(*samples)[0]), 0) # (c_bar, D_1), ..., (c_bar, D_K)\n",
    "        # \\frac1K \\sum_{i = 1}^K \\delta_i, where \\delta_i = L_\\theta(c, D_i) - L_\\theta(\\bar{c}, D_i)\n",
    "        delta_ = th.mean(approx(c_d) - approx(c_bar_d), 0)\n",
    "        \n",
    "        total_delta += abs(float(delta))\n",
    "        total_delta_ += abs(float(delta_))\n",
    "\n",
    "        # \\arg \\min_\\theta (\\delta - \\frac1K \\sum_{i = 1}^K \\delta_i)^2\n",
    "        mse = MSELoss()(delta_, delta)\n",
    "        approx_optim.zero_grad()\n",
    "        mse.backward()\n",
    "        approx_optim.step()\n",
    "        total_mse += float(mse)\n",
    "    \n",
    "#     if (i + 1) % 100 == 0:\n",
    "#         print('[iteration %d]mse: %f, delta: %f, delta_: %f' % (\n",
    "#             (i + 1), total_mse / (j + 1), total_delta / (j + 1), total_delta_ / (j + 1)))\n",
    "        \n",
    "    samples = [sample(train_data, train_labels) for _ in range(K)] # D_1, ..., D_K\n",
    "    c_d = th.cat(map(lambda X: data(c, X), zip(*samples)[0]), 0) # (c, D_1), ..., (c, D_K)\n",
    "    # \\arg \\min_c \\frac1K \\sum_{i = 1}^K L_\\theta (c, D_i)\n",
    "    objective = -th.mean(approx(c_d))\n",
    "    c_optim.zero_grad()\n",
    "    objective.backward()\n",
    "    c_optim.step()\n",
    "    \n",
    "    if (i + 1) % 1000 == 0:\n",
    "        y_bar = predict(c, test_data)\n",
    "        f1 = f_beta(y_bar, test_labels)\n",
    "        print('[iteration %d]objective: %f, f1: %f' % ((i + 1), float(objective), float(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bar = predict(c, test_data)\n",
    "accuracy_ = accuracy(y_bar, test_labels)\n",
    "precision_ = precision(y_bar, test_labels)\n",
    "recall_ = recall(y_bar, test_labels)\n",
    "f1 = f_beta(y_bar, test_labels)\n",
    "print('accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy_, precision_, recall_, f1))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
