{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.loss import CrossEntropyLoss, MSELoss\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "import my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_TRAIN, N_TEST = 0, 0\n",
    "train_data, train_labels, test_data, test_labels = my.unbalanced_dataset(\n",
    "    'MNIST', N_TRAIN, N_TEST, pca=False, p=[0, 1, 10])\n",
    "\n",
    "train_data_np, train_labels_np, test_data_np, test_labels_np = \\\n",
    "    train_data, train_labels, test_data, test_labels\n",
    "    \n",
    "train_data = th.from_numpy(train_data).float()\n",
    "train_labels = th.from_numpy(train_labels).long()\n",
    "test_data = th.from_numpy(test_data).float()\n",
    "test_labels = th.from_numpy(test_labels).long()\n",
    "\n",
    "cuda = True\n",
    "if cuda:\n",
    "    th.cuda.set_device(3)\n",
    "    train_data, train_labels = train_data.cuda(), train_labels.cuda()\n",
    "    test_data, test_labels = test_data.cuda(), test_labels.cuda()\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = \\\n",
    "    map(Variable, (train_data, train_labels, test_data, test_labels))\n",
    "\n",
    "N_FEATURES = train_data.size()[1]\n",
    "N_CLASSES = int(train_labels.max() - train_labels.min() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 1000]cross-entropy loss: 0.344358, accuracy: 0.908700\n",
      "[iteration 2000]cross-entropy loss: 0.247047, accuracy: 0.951150\n",
      "[iteration 3000]cross-entropy loss: 0.193739, accuracy: 0.968400\n",
      "[iteration 4000]cross-entropy loss: 0.160498, accuracy: 0.976417\n",
      "[iteration 5000]cross-entropy loss: 0.138098, accuracy: 0.980583\n",
      "[iteration 6000]cross-entropy loss: 0.122109, accuracy: 0.983300\n",
      "[iteration 7000]cross-entropy loss: 0.110164, accuracy: 0.984867\n",
      "[iteration 8000]cross-entropy loss: 0.100913, accuracy: 0.986000\n",
      "[iteration 9000]cross-entropy loss: 0.093537, accuracy: 0.986817\n",
      "[iteration 10000]cross-entropy loss: 0.087518, accuracy: 0.987383\n"
     ]
    }
   ],
   "source": [
    "c = nn.Linear(N_FEATURES, N_CLASSES)\n",
    "if cuda:\n",
    "    c.cuda()\n",
    "optim = SGD(c.parameters(), lr=0.001)\n",
    "N_ITERATIONS = 10000\n",
    "for i in range(N_ITERATIONS):\n",
    "    loss = CrossEntropyLoss()(c(train_data), train_labels)\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        accuracy = my.accuracy(my.predict(c, train_data), train_labels)\n",
    "        print('[iteration %d]cross-entropy loss: %f, accuracy: %f' % ((i + 1), float(loss), float(accuracy)))\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.986600, precision: 0.951637, recall: 0.975291, f1: 0.963068\n"
     ]
    }
   ],
   "source": [
    "y_bar = my.predict(c, test_data)\n",
    "accuracy = my.accuracy(y_bar, test_labels)\n",
    "precision = my.nd_precision(y_bar, test_labels, N_CLASSES)\n",
    "recall = my.nd_recall(y_bar, test_labels, N_CLASSES)\n",
    "f1 = my.nd_f_beta(y_bar, test_labels, N_CLASSES)\n",
    "print('accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy, precision, recall, f1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "Let $c$ be a classifier and $D=\\{(X_1, y_1),...,(X_N, y_N)\\}$ be the set of training data. In order to minimize $L(c, D)$, where $L$ is a non-decomposable loss function, we introduce $L_\\theta$, a parameterized approximation of $L(c, D)$, and update $c$ as follows:\n",
    "\n",
    "1. Compute $\\delta = L(c, D)-L(\\bar{c},D)$, where $\\bar{c}$ is obtained by stochastically perturbing the parameters of $c$\n",
    "\n",
    "2. Randomly sample $K$ subsets, $D_1, ..., D_K$, of $D$ (these subsets may vary in cardinality)\n",
    "\n",
    "3. Minimize $(\\delta - \\frac1K \\sum_{i = 1}^K \\delta_i)^2$ with respect to $\\theta$, where $\\delta_i = L_\\theta(c, D_i) - L_\\theta(\\bar{c}, D_i)$\n",
    "\n",
    "4. Repeat 1, 2, and 3 several times until $L_\\theta$ becomes a satisfactory approximation of $L$ near $c$\n",
    "\n",
    "5. Randomly sample $K'$ subsets, $D_1, ..., D_K'$, of $D$ and let $c \\leftarrow c - \\alpha \\sum_{i = 1}^K \\frac{\\partial L_\\theta}{\\partial c} (c, D_i)$, where $\\alpha$ is a positive learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 64\n",
    "\n",
    "def L(classifier, X, y):\n",
    "    y_bar = my.predict(classifier, X)\n",
    "    return my.nd_f_beta(y_bar, y, N_CLASSES)\n",
    "\n",
    "def forward(classifier, pair):\n",
    "    X, y = pair\n",
    "    y = my.onehot(y, N_CLASSES)\n",
    "    y_bar = F.softmax(classifier(X), 1)\n",
    "    return th.cat((y, y_bar), 1).view(1, -1)\n",
    "    \n",
    "def sample(K=10):\n",
    "    samples = [my.sample_subset(train_data_np, train_labels_np, SAMPLE_SIZE) for k in range(K)]\n",
    "    if cuda:\n",
    "        samples = [(X.cuda(), y.cuda()) for (X, y) in samples]\n",
    "    return [(Variable(X), Variable(y)) for (X, y) in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3194884955883026"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = nn.Linear(N_FEATURES, N_CLASSES)\n",
    "approx = my.MLP(((N_CLASSES +  N_CLASSES) * SAMPLE_SIZE,) + (1024,) * 3 +(1,), F.relu)\n",
    "\n",
    "if cuda:\n",
    "    c.cuda()\n",
    "    approx.cuda()\n",
    "\n",
    "c_optim = SGD(c.parameters(), 0.1, momentum=0.9)\n",
    "approx_optim = SGD(approx.parameters(), 0.1, momentum=0.9)\n",
    "# c_optim = Adam(c.parameters(), 1e-3)\n",
    "# approx_optim = Adam(approx.parameters(), 1e-3)\n",
    "\n",
    "float(my.nd_f_beta(my.predict(c, test_data), test_labels, N_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 100]mse: 0.009566, objective: -0.493887, f1: 0.594356\n",
      "[iteration 200]mse: 0.000256, objective: -0.409504, f1: 0.637162\n",
      "[iteration 300]mse: 0.006273, objective: -0.573395, f1: 0.660234\n",
      "[iteration 400]mse: 0.000182, objective: -0.588699, f1: 0.591669\n",
      "[iteration 500]mse: 0.000240, objective: -0.792452, f1: 0.635765\n",
      "[iteration 600]mse: 0.001925, objective: -2.011851, f1: 0.664736\n",
      "[iteration 700]mse: 0.000156, objective: -2.190856, f1: 0.637485\n",
      "[iteration 800]mse: 0.000008, objective: -3.005857, f1: 0.878504\n",
      "[iteration 900]mse: 0.000014, objective: -4.192799, f1: 0.883663\n",
      "[iteration 1000]mse: 0.000091, objective: -4.566614, f1: 0.896868\n",
      "[iteration 1100]mse: 0.000027, objective: -4.760266, f1: 0.922461\n",
      "[iteration 1200]mse: 0.000024, objective: -5.938343, f1: 0.944596\n",
      "[iteration 1300]mse: 0.000008, objective: -6.220542, f1: 0.956549\n",
      "[iteration 1400]mse: 0.000012, objective: -6.416841, f1: 0.954157\n",
      "[iteration 1500]mse: 0.000362, objective: -6.357320, f1: 0.965065\n",
      "[iteration 1600]mse: 0.000195, objective: -6.759338, f1: 0.961806\n",
      "[iteration 1700]mse: 0.000195, objective: -6.572813, f1: 0.968434\n",
      "[iteration 1800]mse: 0.000005, objective: -6.362418, f1: 0.969824\n",
      "[iteration 1900]mse: 0.000121, objective: -6.776310, f1: 0.968899\n",
      "[iteration 2000]mse: 0.000112, objective: -6.752350, f1: 0.971062\n"
     ]
    }
   ],
   "source": [
    "STD = 0.1\n",
    "OUTER = 2000\n",
    "INNER = 10\n",
    "\n",
    "for i in range(OUTER):\n",
    "    for j in range(INNER):\n",
    "        c_bar = my.perturb(c, STD)\n",
    "        delta = L(c, train_data, train_labels) - L(c_bar, train_data, train_labels)\n",
    "\n",
    "        samples = sample()\n",
    "        y = th.cat(tuple(map(lambda x: forward(c, x), samples)), 0)\n",
    "        y_bar = th.cat(tuple(map(lambda x: forward(c_bar, x), samples)), 0)\n",
    "        delta_ = th.mean(approx(y) - approx(y_bar), 0)\n",
    "        \n",
    "        mse = MSELoss()(delta_, delta)\n",
    "        approx_optim.zero_grad()\n",
    "        mse.backward()\n",
    "        approx_optim.step()\n",
    "    \n",
    "    samples = sample()\n",
    "    y = th.cat(tuple(map(lambda x: forward(c, x), samples)), 0)\n",
    "    objective = -th.mean(approx(y))\n",
    "    c_optim.zero_grad()\n",
    "    objective.backward()\n",
    "    c_optim.step()\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        y_bar = my.predict(c, test_data)\n",
    "        f1 = my.nd_f_beta(y_bar, test_labels, N_CLASSES)\n",
    "        print('[iteration %d]mse: %f, objective: %f, f1: %f' % ((i + 1), float(mse), float(objective), float(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.989700, precision: 0.967909, recall: 0.974280, f1: 0.971062\n"
     ]
    }
   ],
   "source": [
    "y_bar = my.predict(c, test_data)\n",
    "accuracy = my.accuracy(y_bar, test_labels)\n",
    "precision = my.nd_precision(y_bar, test_labels, N_CLASSES)\n",
    "recall = my.nd_recall(y_bar, test_labels, N_CLASSES)\n",
    "f1 = my.nd_f_beta(y_bar, test_labels, N_CLASSES)\n",
    "print('accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy, precision, recall, f1))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
