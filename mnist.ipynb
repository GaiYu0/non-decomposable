{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.loss import CrossEntropyLoss, MSELoss\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "import my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_TRAIN, N_TEST = 0, 0\n",
    "train_data, train_labels, test_data, test_labels = my.unbalanced_dataset(\n",
    "    'MNIST', N_TRAIN, N_TEST, pca=False, p=[])\n",
    "\n",
    "train_data_np, train_labels_np, test_data_np, test_labels_np = \\\n",
    "    train_data, train_labels, test_data, test_labels\n",
    "    \n",
    "train_data = th.from_numpy(train_data).float()\n",
    "train_labels = th.from_numpy(train_labels).long()\n",
    "test_data = th.from_numpy(test_data).float()\n",
    "test_labels = th.from_numpy(test_labels).long()\n",
    "\n",
    "cuda = True\n",
    "if cuda:\n",
    "    th.cuda.set_device(3)\n",
    "    train_data, train_labels = train_data.cuda(), train_labels.cuda()\n",
    "    test_data, test_labels = test_data.cuda(), test_labels.cuda()\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = \\\n",
    "    map(Variable, (train_data, train_labels, test_data, test_labels))\n",
    "\n",
    "N_FEATURES = train_data.size()[1]\n",
    "N_CLASSES = int(train_labels.max() - train_labels.min() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 1000]cross-entropy loss: 0.236208, accuracy: 0.934467\n",
      "[iteration 2000]cross-entropy loss: 0.219137, accuracy: 0.939167\n",
      "[iteration 3000]cross-entropy loss: 0.211611, accuracy: 0.941383\n",
      "[iteration 4000]cross-entropy loss: 0.207340, accuracy: 0.942767\n",
      "[iteration 5000]cross-entropy loss: 0.204689, accuracy: 0.943767\n",
      "[iteration 6000]cross-entropy loss: 0.203005, accuracy: 0.943933\n",
      "[iteration 7000]cross-entropy loss: 0.201929, accuracy: 0.944083\n",
      "[iteration 8000]cross-entropy loss: 0.201244, accuracy: 0.944250\n",
      "[iteration 9000]cross-entropy loss: 0.200819, accuracy: 0.944433\n",
      "[iteration 10000]cross-entropy loss: 0.200560, accuracy: 0.944600\n"
     ]
    }
   ],
   "source": [
    "c = nn.Linear(N_FEATURES, N_CLASSES)\n",
    "if cuda:\n",
    "    c.cuda()\n",
    "optim = Adam(c.parameters(), lr=0.001)\n",
    "N_ITERATIONS = 10000\n",
    "for i in range(N_ITERATIONS):\n",
    "    loss = CrossEntropyLoss()(c(train_data), train_labels)\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        accuracy = my.accuracy(my.predict(c, train_data), train_labels)\n",
    "        print('[iteration %d]cross-entropy loss: %f, accuracy: %f' % ((i + 1), float(loss), float(accuracy)))\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.918700, precision: 0.917962, recall: 0.917628, f1: 0.917718\n"
     ]
    }
   ],
   "source": [
    "y_bar = my.predict(c, test_data)\n",
    "accuracy = my.accuracy(y_bar, test_labels)\n",
    "precision = my.nd_precision(y_bar, test_labels, N_CLASSES)\n",
    "recall = my.nd_recall(y_bar, test_labels, N_CLASSES)\n",
    "f1 = my.nd_f_beta(y_bar, test_labels, N_CLASSES)\n",
    "print('accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy, precision, recall, f1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "Let $c$ be a classifier and $D=\\{(X_1, y_1),...,(X_N, y_N)\\}$ be the set of training data. In order to minimize $L(c, D)$, where $L$ is a non-decomposable loss function, we introduce $L_\\theta$, a parameterized approximation of $L(c, D)$, and update $c$ as follows:\n",
    "\n",
    "1. Compute $\\delta = L(c, D)-L(\\bar{c},D)$, where $\\bar{c}$ is obtained by stochastically perturbing the parameters of $c$\n",
    "\n",
    "2. Randomly sample $K$ subsets, $D_1, ..., D_K$, of $D$ (these subsets may vary in cardinality)\n",
    "\n",
    "3. Minimize $(\\delta - \\frac1K \\sum_{i = 1}^K \\delta_i)^2$ with respect to $\\theta$, where $\\delta_i = L_\\theta(c, D_i) - L_\\theta(\\bar{c}, D_i)$\n",
    "\n",
    "4. Repeat 1, 2, and 3 several times until $L_\\theta$ becomes a satisfactory approximation of $L$ near $c$\n",
    "\n",
    "5. Randomly sample $K'$ subsets, $D_1, ..., D_K'$, of $D$ and let $c \\leftarrow c - \\alpha \\sum_{i = 1}^K \\frac{\\partial L_\\theta}{\\partial c} (c, D_i)$, where $\\alpha$ is a positive learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 64\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def L(classifier, X, y):\n",
    "    y_bar = my.predict(classifier, X)\n",
    "    return my.nd_f_beta(y_bar, y, N_CLASSES)\n",
    "\n",
    "def forward(classifier, pair):\n",
    "    X, y = pair\n",
    "    y = my.onehot(y, N_CLASSES)\n",
    "    y_bar = F.softmax(classifier(X), 1)\n",
    "    return th.cat((y, y_bar), 1).view(1, -1)\n",
    "    \n",
    "def sample():\n",
    "    samples = [my.sample_subset(train_data_np, train_labels_np, SAMPLE_SIZE) for k in range(BATCH_SIZE)]\n",
    "    if cuda:\n",
    "        samples = [(X.cuda(), y.cuda()) for (X, y) in samples]\n",
    "    return [(Variable(X), Variable(y)) for (X, y) in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10084491968154907"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = nn.Linear(N_FEATURES, N_CLASSES)\n",
    "# critic = my.MLP(((N_CLASSES +  N_CLASSES) * SAMPLE_SIZE,) + (1024,) * 3 +(1,), F.relu)\n",
    "critic = my.RN(SAMPLE_SIZE, 2 * N_CLASSES, (1024,) * 3 + (1,), F.relu)\n",
    "\n",
    "if cuda:\n",
    "    c.cuda()\n",
    "    critic.cuda()\n",
    "\n",
    "# c_optim = SGD(c.parameters(), 0.1, momentum=0.9)\n",
    "# critic_optim = SGD(critic.parameters(), 0.1, momentum=0.9)\n",
    "c_optim = Adam(c.parameters(), 1e-3)\n",
    "critic_optim = Adam(critic.parameters(), 1e-3)\n",
    "\n",
    "float(my.nd_f_beta(my.predict(c, test_data), test_labels, N_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 10]mse: 0.000616, objective: -0.668115, f1: 0.089080\n",
      "[iteration 20]mse: 0.000251, objective: -0.445720, f1: 0.093592\n",
      "[iteration 30]mse: 0.000299, objective: -0.063844, f1: 0.126089\n",
      "[iteration 40]mse: 0.000223, objective: -0.326191, f1: 0.154079\n",
      "[iteration 50]mse: 0.000038, objective: 0.502596, f1: 0.235320\n",
      "[iteration 60]mse: 0.000053, objective: 0.263235, f1: 0.298056\n",
      "[iteration 70]mse: 0.001560, objective: -0.245873, f1: 0.418190\n",
      "[iteration 80]mse: 0.001454, objective: -0.030150, f1: 0.531991\n",
      "[iteration 90]mse: 0.000121, objective: 0.055552, f1: 0.573501\n",
      "[iteration 100]mse: 0.000225, objective: -0.239900, f1: 0.597914\n",
      "[iteration 110]mse: 0.000001, objective: -0.043828, f1: 0.617740\n",
      "[iteration 120]mse: 0.000167, objective: 0.564130, f1: 0.632668\n",
      "[iteration 130]mse: 0.000074, objective: -0.621435, f1: 0.649655\n",
      "[iteration 140]mse: 0.000299, objective: -0.881000, f1: 0.751355\n",
      "[iteration 150]mse: 0.000030, objective: 0.126243, f1: 0.765008\n",
      "[iteration 160]mse: 0.000001, objective: -0.925938, f1: 0.769964\n",
      "[iteration 170]mse: 0.000021, objective: -0.662211, f1: 0.775735\n",
      "[iteration 180]mse: 0.000050, objective: -0.777020, f1: 0.783993\n",
      "[iteration 190]mse: 0.000016, objective: -0.845083, f1: 0.791924\n",
      "[iteration 200]mse: 0.000039, objective: -0.747373, f1: 0.812954\n",
      "[iteration 210]mse: 0.000006, objective: -0.808200, f1: 0.847932\n",
      "[iteration 220]mse: 0.000030, objective: -0.964240, f1: 0.870490\n",
      "[iteration 230]mse: 0.000018, objective: -0.818232, f1: 0.882450\n",
      "[iteration 240]mse: 0.000003, objective: -1.114441, f1: 0.888444\n",
      "[iteration 250]mse: 0.000009, objective: -0.987265, f1: 0.890761\n",
      "[iteration 260]mse: 0.000162, objective: -0.585658, f1: 0.894642\n",
      "[iteration 270]mse: 0.000017, objective: -0.953057, f1: 0.897388\n",
      "[iteration 280]mse: 0.000005, objective: -1.063427, f1: 0.897941\n",
      "[iteration 290]mse: 0.000004, objective: -0.794209, f1: 0.900098\n",
      "[iteration 300]mse: 0.000000, objective: -1.047530, f1: 0.901942\n",
      "[iteration 310]mse: 0.000034, objective: -1.159882, f1: 0.902682\n",
      "[iteration 320]mse: 0.000011, objective: -1.185472, f1: 0.903085\n",
      "[iteration 330]mse: 0.000010, objective: -1.266386, f1: 0.904312\n",
      "[iteration 340]mse: 0.000006, objective: -1.463490, f1: 0.906566\n",
      "[iteration 350]mse: 0.000033, objective: -1.197068, f1: 0.907346\n",
      "[iteration 360]mse: 0.000001, objective: -1.292762, f1: 0.907641\n",
      "[iteration 370]mse: 0.000007, objective: -1.359372, f1: 0.908271\n",
      "[iteration 380]mse: 0.000051, objective: -1.255663, f1: 0.910151\n",
      "[iteration 390]mse: 0.000038, objective: -1.266273, f1: 0.910424\n",
      "[iteration 400]mse: 0.000003, objective: -1.083120, f1: 0.911563\n",
      "[iteration 410]mse: 0.000002, objective: -1.138199, f1: 0.911460\n",
      "[iteration 420]mse: 0.000118, objective: -1.220974, f1: 0.912033\n",
      "[iteration 430]mse: 0.000001, objective: -1.298990, f1: 0.912671\n",
      "[iteration 440]mse: 0.000041, objective: -0.901958, f1: 0.912156\n",
      "[iteration 450]mse: 0.000084, objective: -1.118522, f1: 0.912065\n",
      "[iteration 460]mse: 0.000017, objective: -0.923422, f1: 0.912836\n",
      "[iteration 470]mse: 0.000006, objective: -1.239388, f1: 0.912995\n",
      "[iteration 480]mse: 0.000058, objective: -1.782927, f1: 0.912441\n",
      "[iteration 490]mse: 0.000004, objective: -1.587108, f1: 0.912380\n",
      "[iteration 500]mse: 0.000010, objective: -1.348356, f1: 0.912259\n"
     ]
    }
   ],
   "source": [
    "STD = 0.05\n",
    "OUTER = 500\n",
    "INNER = 10\n",
    "\n",
    "for i in range(OUTER):\n",
    "    for j in range(INNER):\n",
    "        c_bar = my.perturb(c, STD)\n",
    "        delta = L(c, train_data, train_labels) - L(c_bar, train_data, train_labels)\n",
    "\n",
    "        samples = sample()\n",
    "        y = th.cat(tuple(map(lambda x: forward(c, x), samples)), 0)\n",
    "        y_bar = th.cat(tuple(map(lambda x: forward(c_bar, x), samples)), 0)\n",
    "        delta_ = th.mean(critic(y) - critic(y_bar), 0)\n",
    "        \n",
    "        mse = MSELoss()(delta_, delta)\n",
    "        critic_optim.zero_grad()\n",
    "        mse.backward()\n",
    "        critic_optim.step()\n",
    "    \n",
    "    samples = sample()\n",
    "    y = th.cat(tuple(map(lambda x: forward(c, x), samples)), 0)\n",
    "    objective = -th.mean(critic(y))\n",
    "    c_optim.zero_grad()\n",
    "    objective.backward()\n",
    "    c_optim.step()\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        y_bar = my.predict(c, test_data)\n",
    "        f1 = my.nd_f_beta(y_bar, test_labels, N_CLASSES)\n",
    "        print('[iteration %d]mse: %f, objective: %f, f1: %f' % ((i + 1), float(mse), float(objective), float(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.913600, precision: 0.912556, recall: 0.912634, f1: 0.912259\n"
     ]
    }
   ],
   "source": [
    "y_bar = my.predict(c, test_data)\n",
    "accuracy = my.accuracy(y_bar, test_labels)\n",
    "precision = my.nd_precision(y_bar, test_labels, N_CLASSES)\n",
    "recall = my.nd_recall(y_bar, test_labels, N_CLASSES)\n",
    "f1 = my.nd_f_beta(y_bar, test_labels, N_CLASSES)\n",
    "print('accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy, precision, recall, f1))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
