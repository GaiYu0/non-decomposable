{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import MSELoss\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, D, nonlinear):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(D[i], D[i + 1]) for i in range(len(D) - 1)])\n",
    "        self.nonlinear = nonlinear\n",
    "        self.expose = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dim != 2:\n",
    "            x = x.view(x.size()[0], -1)\n",
    "        for i, linear in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            if i < len(self.linears) - 1:\n",
    "                x = self.nonlinear(x)\n",
    "        return F.log_softmax(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 100]nll loss: 0.475699, accuracy 0.859375\n",
      "[iteration 200]nll loss: 0.216485, accuracy 0.953125\n",
      "[iteration 300]nll loss: 0.240146, accuracy 0.890625\n",
      "[iteration 400]nll loss: 0.467797, accuracy 0.859375\n",
      "[iteration 500]nll loss: 0.473764, accuracy 0.875000\n",
      "[iteration 600]nll loss: 0.388439, accuracy 0.890625\n",
      "[iteration 700]nll loss: 0.447029, accuracy 0.906250\n",
      "[iteration 800]nll loss: 0.273371, accuracy 0.937500\n",
      "[iteration 900]nll loss: 0.167352, accuracy 0.968750\n",
      "[epoch 1]accuracy: 0.915000, precision: 0.914516, recall: 0.913974, f1: 0.913725\n"
     ]
    }
   ],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.131,), (0.308,))])\n",
    "mnist_train = MNIST('MNIST/', True, transform=trans)\n",
    "train_loader = DataLoader(mnist_train, 64, True)\n",
    "mnist_test = MNIST('MNIST/', False, transform=trans)\n",
    "test_loader = DataLoader(mnist_test, 1024)\n",
    "\n",
    "mlp = MLP((28 * 28, 10), th.tanh)\n",
    "optim = Adam(mlp.parameters(), lr=0.001)\n",
    "\n",
    "N_EPOCHS = 1\n",
    "for e in range(N_EPOCHS):\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        X, y = Variable(X), Variable(y)\n",
    "        optim.zero_grad()\n",
    "        log_softmax = mlp(X)\n",
    "        loss = F.nll_loss(log_softmax, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            accuracy = my.accuracy(my.predict(mlp, X), y)\n",
    "            print('[iteration %d]nll loss: %f, accuracy %f' % (i + 1, float(loss), float(accuracy)))\n",
    "        \n",
    "    accuracy, precision, recall, f1 = my.global_stats(mlp, test_loader, (\n",
    "        my.accuracy,\n",
    "        my.nd_curry(my.nd_precision, 10),\n",
    "        my.nd_curry(my.nd_recall, 10),\n",
    "        my.nd_curry(my.nd_f_beta, 10)))\n",
    "    print('[epoch %d]accuracy: %f, precision: %f, recall: %f, f1: %f' % (\n",
    "        e + 1, float(accuracy), float(precision), float(recall), float(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08324562013149261"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_TRAIN, N_TEST = 1000, 1000\n",
    "# train_data, train_labels, test_data, test_labels = my.unbalanced_mnist(N_TRAIN, N_TEST, pca=True)\n",
    "# train_loader = DataLoader(th.utils.data.TensorDataset(train_data.data, train_labels.data), train_data.size()[0])\n",
    "train_data = my.th_normalize(mnist_train.train_data).view(mnist_train.train_data.size()[0], -1)\n",
    "train_labels = mnist_train.train_labels\n",
    "# train_labels = train_labels.numpy()\n",
    "# train_labels[train_labels != 0] = 1\n",
    "# train_labels = th.from_numpy(train_labels)\n",
    "train_loader = DataLoader(\n",
    "    th.utils.data.TensorDataset(*my.sample_subset(train_data, train_labels, N_TRAIN, False)), N_TRAIN)\n",
    "\n",
    "d = int(train_labels.max() - train_labels.min() + 1)\n",
    "c = MLP((train_data.size()[1], d), th.tanh) # the classifier\n",
    "c_optim = SGD(c.parameters(), 0.01)\n",
    "\n",
    "SAMPLE_SIZE = 16\n",
    "D = (train_data.size()[1] + d + d) * SAMPLE_SIZE + sum(p.numel() for p in c.parameters())\n",
    "approx = nn.Sequential(\n",
    "    nn.Linear(D, 256),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(256, 1)\n",
    ") # L_\\theta\n",
    "approx_optim = SGD(approx.parameters(), 0.01)\n",
    "float(my.global_stats(c, train_loader, my.nd_curry(my.nd_f_beta, d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 10]mse: 0.005802, f1: 0.084248\n",
      "[iteration 20]mse: 0.002567, f1: 0.083122\n",
      "[iteration 30]mse: 0.000086, f1: 0.082517\n",
      "[iteration 40]mse: 0.005454, f1: 0.083266\n",
      "[iteration 50]mse: 0.000002, f1: 0.082339\n",
      "[iteration 60]mse: 0.000501, f1: 0.082291\n",
      "[iteration 70]mse: 0.004007, f1: 0.086820\n",
      "[iteration 80]mse: 0.000006, f1: 0.088180\n",
      "[iteration 90]mse: 0.000291, f1: 0.089575\n",
      "[iteration 100]mse: 0.000137, f1: 0.091936\n",
      "[iteration 110]mse: 0.000001, f1: 0.089893\n",
      "[iteration 120]mse: 0.000015, f1: 0.089094\n",
      "[iteration 130]mse: 0.009502, f1: 0.090232\n",
      "[iteration 140]mse: 0.000210, f1: 0.091078\n",
      "[iteration 150]mse: 0.000024, f1: 0.092304\n",
      "[iteration 160]mse: 0.000030, f1: 0.094909\n",
      "[iteration 170]mse: 0.001438, f1: 0.094989\n",
      "[iteration 180]mse: 0.000172, f1: 0.095354\n",
      "[iteration 190]mse: 0.001220, f1: 0.096929\n",
      "[iteration 200]mse: 0.000019, f1: 0.096896\n",
      "[iteration 210]mse: 0.000203, f1: 0.097210\n",
      "[iteration 220]mse: 0.000034, f1: 0.097107\n",
      "[iteration 230]mse: 0.000372, f1: 0.097958\n",
      "[iteration 240]mse: 0.006164, f1: 0.096472\n",
      "[iteration 250]mse: 0.003452, f1: 0.096686\n",
      "[iteration 260]mse: 0.001042, f1: 0.094742\n",
      "[iteration 270]mse: 0.000441, f1: 0.098964\n",
      "[iteration 280]mse: 0.002156, f1: 0.099882\n",
      "[iteration 290]mse: 0.001075, f1: 0.099863\n",
      "[iteration 300]mse: 0.000052, f1: 0.100007\n",
      "[iteration 310]mse: 0.000625, f1: 0.101013\n",
      "[iteration 320]mse: 0.000102, f1: 0.102092\n",
      "[iteration 330]mse: 0.000410, f1: 0.104045\n",
      "[iteration 340]mse: 0.001367, f1: 0.103033\n",
      "[iteration 350]mse: 0.001550, f1: 0.105023\n",
      "[iteration 360]mse: 0.000020, f1: 0.104374\n",
      "[iteration 370]mse: 0.000959, f1: 0.105426\n",
      "[iteration 380]mse: 0.000603, f1: 0.105439\n",
      "[iteration 390]mse: 0.000377, f1: 0.104631\n",
      "[iteration 400]mse: 0.000707, f1: 0.105582\n",
      "[iteration 410]mse: 0.000494, f1: 0.106783\n",
      "[iteration 420]mse: 0.000009, f1: 0.107800\n",
      "[iteration 430]mse: 0.001401, f1: 0.109783\n",
      "[iteration 440]mse: 0.000035, f1: 0.109530\n"
     ]
    }
   ],
   "source": [
    "OUTER = 50000\n",
    "INNER = 10\n",
    "STD = 5\n",
    "K = 10\n",
    "\n",
    "L = lambda c, loader: my.global_stats(c, loader, my.nd_curry(my.nd_f_beta, d))\n",
    "def data(classifier, pair):\n",
    "    X, y = pair\n",
    "    y_bar = classifier(X)\n",
    "    exposure = th.cat((X, my.onehot(y.view(y.size()[0], 1), d), y_bar), 1).view(1, -1)\n",
    "    parameters = list(map(lambda p: p.view(1, -1), classifier.parameters()))\n",
    "    return th.cat([exposure] + parameters, 1)\n",
    "\n",
    "for i in range(OUTER):\n",
    "    total_mse = 0\n",
    "    total_delta, total_delta_ = 0, 0\n",
    "    for j in range(INNER):\n",
    "        c_bar = my.perturb(c, STD)\n",
    "        delta = L(c, train_loader) - L(c_bar, train_loader)\n",
    "        \n",
    "        samples = [my.sample_subset(train_data, train_labels, SAMPLE_SIZE) for k in range(K)]\n",
    "        c_d = th.cat(map(lambda X: data(c, X), samples), 0)\n",
    "        c_bar_d = th.cat(map(lambda X: data(c_bar, X), samples), 0)\n",
    "        delta_bar = th.mean(approx(c_d) - approx(c_bar_d), 0)\n",
    "        \n",
    "        mse = MSELoss()(delta_bar, delta)\n",
    "        approx_optim.zero_grad()\n",
    "        mse.backward()\n",
    "        approx_optim.step()\n",
    "        \n",
    "    samples = [my.sample_subset(train_data, train_labels, SAMPLE_SIZE) for k in range(K)]\n",
    "    c_d = th.cat(map(lambda X: data(c, X), samples), 0)\n",
    "    f1_bar = -th.mean(approx(c_d))\n",
    "    c_optim.zero_grad()\n",
    "    f1_bar.backward()\n",
    "    c_optim.step()\n",
    "\n",
    "    if (i + 1) % 10 == 0:\n",
    "        f1 = L(c, train_loader)\n",
    "        print('[iteration %d]mse: %f, f1: %f' % ((i + 1), float(mse), float(f1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
