{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import copy\n",
    "from threading import Condition, Thread\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.loss import CrossEntropyLoss, MSELoss\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_TRAIN, N_TEST = 0, 0\n",
    "BATCH_SIZE = 64\n",
    "cuda = True\n",
    "\n",
    "# train_data, train_labels, test_data, test_labels = my.unbalanced_cifar10(N_TRAIN, N_TEST, p=[0, 1, 10])\n",
    "train_data, train_labels, test_data, test_labels = my.unbalanced_cifar10(N_TRAIN, N_TEST, p=[])\n",
    "\n",
    "train_data_np, train_labels_np, test_data_np, test_labels_np = \\\n",
    "    train_data, train_labels, test_data, test_labels\n",
    "    \n",
    "train_data = th.from_numpy(train_data).float()\n",
    "train_labels = th.from_numpy(train_labels).long()\n",
    "test_data = th.from_numpy(test_data).float()\n",
    "test_labels = th.from_numpy(test_labels).long()\n",
    "\n",
    "\n",
    "if cuda:\n",
    "    th.cuda.set_device(3)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_data, train_labels), BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_data, test_labels), BATCH_SIZE)\n",
    "\n",
    "N_FEATURES = train_data.size()[1]\n",
    "N_CLASSES = int(train_labels.max() - train_labels.min() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, 3, 2, 1)\n",
    "        self.linear = nn.Linear(8, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dim() != 4:\n",
    "            x = x.view(-1, 3, 32, 32)\n",
    "        x = F.tanh(self.conv1(x))\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        x = F.avg_pool2d(x, 8)\n",
    "        x = self.linear(x.view(-1, 8))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# c = my.MLP((N_FEATURES,) + (64,) * 3 + (N_CLASSES,), F.relu)\n",
    "# c_pretrained = CNN(N_CLASSES)\n",
    "# if cuda:\n",
    "#     c_pretrained.cuda()\n",
    "# optim = Adam(c_pretrained.parameters(), lr=0.001)\n",
    "# EPOCHS = 1\n",
    "# for i in range(EPOCHS):\n",
    "#     for x, y in train_loader:\n",
    "#         if cuda:\n",
    "#             x, y = x.cuda(), y.cuda()\n",
    "#         x, y = Variable(x), Variable(y)\n",
    "#         loss = CrossEntropyLoss()(c_pretrained(x), y)\n",
    "#         optim.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "#     accuracy = my.global_stats(c_pretrained, test_loader, my.accuracy)\n",
    "#     print('[epoch %d]cross-entropy loss: %f, accuracy: %f' % ((i + 1), float(loss), float(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nd_stats = [my.accuracy] + [my.nd_curry(stat, N_CLASSES) for stat in (my.nd_precision, my.nd_recall, my.nd_f_beta)]\n",
    "# accuracy, precision, recall, f1 = my.global_stats(c_pretrained, test_loader, nd_stats)\n",
    "# 'accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy, precision, recall, f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(sample_size, batch_size):\n",
    "    samples = [my.sample_subset(train_data_np, train_labels_np, sample_size) for k in range(batch_size)]\n",
    "    if cuda:\n",
    "        samples = [(x.cuda(), y.cuda()) for (x, y) in samples]\n",
    "    return [(Variable(x), Variable(y)) for (x, y) in samples]\n",
    "\n",
    "def perturb_y(y, std):\n",
    "    return y + (lambda x: x.cuda() if cuda else x)(Variable(th.randn(y.size()) * std))\n",
    "\n",
    "def L(y_bar, y):\n",
    "    return th.cat(tuple(my.nd_f_beta(th.max(z_bar, 1)[1], z, N_CLASSES).view(1, 1)\n",
    "                        for z_bar, z in zip(y_bar, y)), 1)\n",
    "\n",
    "def critic_forward(y_bar, y, detach=False):\n",
    "    y_bar = tuple(F.softmax(z_bar, 1) for z_bar in y_bar)\n",
    "    y = tuple(my.onehot(z, N_CLASSES) for z in y)\n",
    "    z = tuple(th.cat((z_bar, z), 1).view(1, -1) for z_bar, z in zip(y_bar, y))\n",
    "    return critic((lambda x: x.detach() if detach else x)(th.cat(z, 0)))\n",
    "\n",
    "def MSE(y_bar, y_perturbed, y, target):\n",
    "    l_bar = critic_forward(y_bar, y, True)\n",
    "    l_perturbed = critic_forward(y_perturbed, y, True)\n",
    "#     return th.mean(th.exp(target ** 2 / 0.1) * ((l_perturbed - l_bar) - target) ** 2)\n",
    "    return th.mean(((l_perturbed - l_bar) - target) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy: 0.100000, precision: 0.010000, recall: 0.100000, f1: 0.018180'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_SIZE = 16\n",
    "\n",
    "th.random.manual_seed(1)\n",
    "th.cuda.manual_seed_all(1)\n",
    "\n",
    "# c = my.MLP((N_FEATURES,) + (512,) * 1 + (N_CLASSES,), F.relu)\n",
    "c = CNN(N_CLASSES)\n",
    "# c.load_state_dict(c_pretrained.state_dict())\n",
    "critic = my.RN(SAMPLE_SIZE, 2 * N_CLASSES, (512,) * 3 + (1,), F.relu)\n",
    "\n",
    "if cuda:\n",
    "    c.cuda()\n",
    "    critic.cuda()\n",
    "\n",
    "# c_optim = SGD(c.parameters(), 0.1, momentum=0.5)\n",
    "# critic_optim = SGD(critic.parameters(), 0.1, momentum=0.5)\n",
    "c_optim = Adam(c.parameters(), 1e-3)\n",
    "critic_optim = Adam(critic.parameters(), 1e-3)\n",
    "\n",
    "nd_stats = [my.accuracy] + [my.nd_curry(stat, N_CLASSES) for stat in (my.nd_precision, my.nd_recall, my.nd_f_beta)]\n",
    "accuracy, precision, recall, f1 = my.global_stats(c, test_loader, nd_stats)\n",
    "'accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy, precision, recall, f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_ITERATIONS = 5\n",
    "N_PERTURBATIONS = 25\n",
    "BATCH_SIZE = 4\n",
    "STD = 1e-1\n",
    "CRITIC_N_ITERATIONS = 10\n",
    "CLASSIFIER_N_ITERATIONS = 25\n",
    "\n",
    "# TODO replay buffer\n",
    "hist = []\n",
    "for i in range(N_ITERATIONS):\n",
    "    hist.append({})\n",
    "    hist[-1]['c_state_dict'] = copy.deepcopy(c.state_dict())\n",
    "\n",
    "    c.eval()\n",
    "    critic.train()\n",
    "    x, y = zip(*sample(SAMPLE_SIZE, BATCH_SIZE))\n",
    "    hist[-1]['x'], hist[-1]['y'] = x, y\n",
    "    y_bar = tuple(map(c, x))\n",
    "    L_bar = L(y_bar, y)\n",
    "    l_bar = critic_forward(y_bar, y, True)\n",
    "\n",
    "    p_list = []\n",
    "    for j in range(N_PERTURBATIONS):\n",
    "        y_perturbed = tuple(perturb_y(z_bar, STD) for z_bar in y_bar) # TODO perturb in simplex\n",
    "        L_perturbed = L(y_perturbed, y)\n",
    "        target = L_perturbed - L_bar\n",
    "        p_list.append((y_perturbed, target))\n",
    "\n",
    "    for j in range(CRITIC_N_ITERATIONS):\n",
    "        mse = 0\n",
    "        for y_perturbed, target in p_list:\n",
    "            mse += MSE(y_bar, y_perturbed, y, target)\n",
    "        mse /= N_PERTURBATIONS\n",
    "        critic_optim.zero_grad()\n",
    "        mse.backward()\n",
    "        critic_optim.step()\n",
    "    hist[-1]['critic_state_dict'] = copy.deepcopy(critic.state_dict())\n",
    "    \n",
    "    c.train()\n",
    "    critic.eval()\n",
    "    c_params = copy.deepcopy(tuple(c.parameters()))\n",
    "    for j in range(CLASSIFIER_N_ITERATIONS):\n",
    "        y_bar = tuple(map(c, x))\n",
    "        objective = -th.mean(critic_forward(y_bar, y))\n",
    "        c_optim.zero_grad()\n",
    "        objective.backward()\n",
    "        c_optim.step()\n",
    "        if any(float(th.max(th.abs(p - q))) > 0.1 for p, q in zip(c_params, c.parameters())):\n",
    "            break\n",
    "    \n",
    "    if (i + 1) % 1 == 0:\n",
    "        f1 = my.global_stats(c, test_loader, my.nd_curry(my.nd_f_beta, N_CLASSES))\n",
    "        print('[iteration %d]mse: %f; f1: %f' % (i + 1, mse, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for h in hist:\n",
    "    c, c_bar = CNN(N_CLASSES), CNN(N_CLASSES)\n",
    "    c.load_state_dict(h['c_state_dict'])\n",
    "    c_bar.load_state_dict(h['c_state_dict'])\n",
    "    critic = my.RN(SAMPLE_SIZE, 2 * N_CLASSES, (512,) * 3 + (1,), F.relu)\n",
    "    critic.load_state_dict(h['critic_state_dict'])\n",
    "    if cuda:\n",
    "        c.cuda()\n",
    "        c_bar.cuda()\n",
    "        critic.cuda()\n",
    "\n",
    "    # TODO gradient in parameter space/simplex\n",
    "    y_bar = tuple(map(c, h['x']))\n",
    "    objective = -th.mean(critic_forward(y_bar, h['y']))\n",
    "    objective.backward()\n",
    "\n",
    "    alpha = np.linspace(0, 1)\n",
    "    f1_list = []\n",
    "    for a in alpha:\n",
    "        for p, p_bar in zip(c.parameters(), c_bar.parameters()):\n",
    "            p_bar.data = p.data + float(a) * p.grad.data\n",
    "        # TODO global f1/batch f1\n",
    "        f1 = my.global_stats(c_bar, test_loader, my.nd_curry(my.nd_f_beta, N_CLASSES))\n",
    "        f1_list.append(float(f1))\n",
    "    \n",
    "    pl.figure()\n",
    "    pl.plot(alpha, f1_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
