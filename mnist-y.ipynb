{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.loss import CrossEntropyLoss, MSELoss\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN, N_TEST = 0, 0\n",
    "BATCH_SIZE = 64\n",
    "cuda = True\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = my.unbalanced_dataset(\n",
    "    'MNIST', N_TRAIN, N_TEST, pca=False, p=[0, 1, 10])\n",
    "\n",
    "train_data_np, train_labels_np, test_data_np, test_labels_np = \\\n",
    "    train_data, train_labels, test_data, test_labels\n",
    "    \n",
    "train_data = th.from_numpy(train_data).float()\n",
    "train_labels = th.from_numpy(train_labels).long()\n",
    "test_data = th.from_numpy(test_data).float()\n",
    "test_labels = th.from_numpy(test_labels).long()\n",
    "\n",
    "if cuda:\n",
    "    th.cuda.set_device(2)\n",
    "    train_data, train_labels = train_data.cuda(), train_labels.cuda()\n",
    "    test_data, test_labels = test_data.cuda(), test_labels.cuda()\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_data, train_labels), BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_data, test_labels), BATCH_SIZE)\n",
    "\n",
    "N_FEATURES = train_data.size()[1]\n",
    "N_CLASSES = int(train_labels.max() - train_labels.min() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, 3, 2, 1)\n",
    "        self.linear = nn.Linear(8, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dim() != 4:\n",
    "            x = x.view(-1, 1, 28, 28)\n",
    "        x = F.tanh(self.conv1(x))\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        x = F.avg_pool2d(x, 7)\n",
    "        x = self.linear(x.view(-1, 8))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = my.MLP((N_FEATURES,) + (64,) * 3 + (N_CLASSES,), F.relu)\n",
    "c = CNN(N_CLASSES)\n",
    "if cuda:\n",
    "    c.cuda()\n",
    "optim = Adam(c.parameters(), lr=0.001)\n",
    "EPOCHS = 10\n",
    "for i in range(EPOCHS):\n",
    "    for X, y in train_loader:\n",
    "        if cuda:\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "        X, y = Variable(X), Variable(y)\n",
    "        loss = CrossEntropyLoss()(c(X), y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    accuracy = my.global_stats(c, test_loader, my.accuracy)\n",
    "    print('[epoch %d]cross-entropy loss: %f, accuracy: %f' % ((i + 1), float(loss), float(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bar = my.predict(c, test_data)\n",
    "accuracy = my.accuracy(y_bar, test_labels)\n",
    "precision = my.nd_precision(y_bar, test_labels, N_CLASSES)\n",
    "recall = my.nd_recall(y_bar, test_labels, N_CLASSES)\n",
    "f1 = my.nd_f_beta(y_bar, test_labels, N_CLASSES)\n",
    "print('accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy, precision, recall, f1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 64\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def L(classifier, X, y):\n",
    "    y_bar = my.predict(classifier, X)\n",
    "    return my.nd_f_beta(y_bar, y, N_CLASSES)\n",
    "\n",
    "def forward(classifier, pair):\n",
    "    X, y = pair\n",
    "    y = my.onehot(y, N_CLASSES)\n",
    "    y_bar = F.softmax(classifier(X), 1)\n",
    "    return th.cat((y, y_bar), 1).view(1, -1)\n",
    "    \n",
    "def sample():\n",
    "    samples = [my.sample_subset(train_data_np, train_labels_np, SAMPLE_SIZE) for k in range(BATCH_SIZE)]\n",
    "    if cuda:\n",
    "        samples = [(X.cuda(), y.cuda()) for (X, y) in samples]\n",
    "    return [(Variable(X), Variable(y)) for (X, y) in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nn.Linear(N_FEATURES, N_CLASSES)\n",
    "# c = CNN(N_CLASSES)\n",
    "# critic = my.MLP(((N_CLASSES +  N_CLASSES) * SAMPLE_SIZE,) + (1024,) * 3 +(1,), F.relu)\n",
    "critic = my.RN(SAMPLE_SIZE, 2 * N_CLASSES, (1024,) * 3 + (1,), F.relu)\n",
    "\n",
    "if cuda:\n",
    "    c.cuda()\n",
    "    critic.cuda()\n",
    "\n",
    "# c_optim = SGD(c.parameters(), 0.1, momentum=0.9)\n",
    "# critic_optim = SGD(critic.parameters(), 0.1, momentum=0.9)\n",
    "c_optim = Adam(c.parameters(), 1e-3)\n",
    "critic_optim = Adam(critic.parameters(), 1e-3)\n",
    "\n",
    "float(my.nd_f_beta(my.predict(c, test_data), test_labels, N_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STD = 0.1\n",
    "OUTER = 500\n",
    "INNER = 10\n",
    "\n",
    "stats = []\n",
    "for i in range(OUTER):\n",
    "    f1 = L(c, train_data, train_labels)\n",
    "    f1_list = []\n",
    "    for j in range(INNER):\n",
    "        c_bar = my.perturb(c, STD)\n",
    "        f1_bar = L(c_bar, train_data, train_labels)\n",
    "        delta = f1 - f1_bar\n",
    "        f1_list.append(float(f1))\n",
    "\n",
    "        samples = sample()\n",
    "        y = th.cat(tuple(map(lambda x: forward(c, x), samples)), 0)\n",
    "        y_bar = th.cat(tuple(map(lambda x: forward(c_bar, x), samples)), 0)\n",
    "        delta_ = th.mean(critic(y) - critic(y_bar), 0)\n",
    "        \n",
    "        mse = MSELoss()(delta_, delta)\n",
    "        critic_optim.zero_grad()\n",
    "        mse.backward()\n",
    "        critic_optim.step()\n",
    "    \n",
    "    samples = sample()\n",
    "    y = th.cat(tuple(map(lambda x: forward(c, x), samples)), 0)\n",
    "    objective = -th.mean(critic(y))\n",
    "    c_optim.zero_grad()\n",
    "    objective.backward()\n",
    "    c_optim.step()\n",
    "    stats.append(f1_list)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        y_bar = my.predict(c, test_data)\n",
    "        f1 = my.nd_f_beta(y_bar, test_labels, N_CLASSES)\n",
    "        print('[iteration %d]mse: %f, objective: %f, f1: %f' % ((i + 1), float(mse), float(objective), float(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bar = my.predict(c, test_data)\n",
    "accuracy = my.accuracy(y_bar, test_labels)\n",
    "precision = my.nd_precision(y_bar, test_labels, N_CLASSES)\n",
    "recall = my.nd_recall(y_bar, test_labels, N_CLASSES)\n",
    "f1 = my.nd_f_beta(y_bar, test_labels, N_CLASSES)\n",
    "print('accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy, precision, recall, f1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(range(sum(map(len, stats))), sum(stats, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(range(len(stats)), tuple(map(max, stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# th.save(critic.state_dict(), 'mnist_critic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
