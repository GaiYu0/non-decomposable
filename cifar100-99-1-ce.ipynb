{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import sklearn.metrics as metrics\n",
    "import tensorboardX as tb\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.modules.loss as loss\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "import data\n",
    "import my\n",
    "import lenet\n",
    "import resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.batch_size = 500\n",
    "args.gpu = 1\n",
    "args.log_every = 1000\n",
    "args.n_iterations = 10000\n",
    "\n",
    "keys = sorted(vars(args).keys())\n",
    "excluded = ('gpu', 'log_every', 'n_iterations')\n",
    "run_id = 'cifar100-relabelled-ce' + '-'.join('%s-%s' % (key, str(getattr(args, key))) for key in keys if key not in excluded)\n",
    "writer = tb.SummaryWriter('runs/' + run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.gpu < 0:\n",
    "    cuda = False\n",
    "else:\n",
    "    cuda = True\n",
    "    th.cuda.set_device(args.gpu)\n",
    "\n",
    "labelling = {(0, 99) : 0, (99, 100) : 1}\n",
    "train_x, train_y, test_x, test_y = data.load_cifar100(labelling, rbg=True, torch=True)\n",
    "\n",
    "train_set = utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = utils.data.DataLoader(train_set, 4096, drop_last=False)\n",
    "test_set = utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = utils.data.DataLoader(test_set, 4096, drop_last=False)\n",
    "\n",
    "loader = data.BalancedDataLoader(train_x, train_y, args.batch_size, cuda)\n",
    "\n",
    "n_classes = int(train_y.max() - train_y.min() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_scores(c, loader):\n",
    "    keys = ('accuracy', 'precision', 'recall', 'f1')\n",
    "    scores = (\n",
    "        metrics.accuracy_score,\n",
    "        lambda y, y_bar: metrics.precision_score(y, y_bar, average='macro'),\n",
    "        lambda y, y_bar: metrics.recall_score(y, y_bar, average='macro'),\n",
    "        lambda y, y_bar: metrics.f1_score(y, y_bar, average='macro'),\n",
    "    )\n",
    "    values = [value.item() for value in my.global_scores(c, loader, scores)]\n",
    "    return collections.OrderedDict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.99\n",
      "precision 0.5\n",
      "recall 0.495\n",
      "f1 0.49748743718592964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaiyu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/gaiyu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# c = lenet.LeNet(3, n_classes)\n",
    "c = resnet.ResNet(18, n_classes)\n",
    "\n",
    "if cuda:\n",
    "    c.cuda()\n",
    "    \n",
    "# optimizer = optim.SGD(c.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer = optim.Adam(c.parameters(), amsgrad=True)\n",
    "\n",
    "for key, value in global_scores(c, test_loader).items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 01000]accuracy 0.992/0.992 | precision 0.639/0.624 | recall 0.878/0.853 | f1 0.702/0.683\n",
      "[iteration 02000]accuracy 0.999/0.992 | precision 0.943/0.704 | recall 0.988/0.844 | f1 0.965/0.756\n",
      "[iteration 03000]accuracy 1.000/0.993 | precision 0.998/0.724 | recall 0.998/0.879 | f1 0.998/0.781\n",
      "[iteration 04000]accuracy 1.000/0.993 | precision 1.000/0.739 | recall 0.998/0.878 | f1 0.999/0.793\n",
      "[iteration 05000]accuracy 1.000/0.993 | precision 0.999/0.719 | recall 0.999/0.876 | f1 0.999/0.777\n",
      "[iteration 06000]accuracy 1.000/0.993 | precision 0.999/0.734 | recall 1.000/0.876 | f1 0.999/0.788\n",
      "[iteration 07000]accuracy 1.000/0.993 | precision 0.999/0.729 | recall 0.999/0.874 | f1 0.999/0.784\n",
      "[iteration 08000]accuracy 1.000/0.993 | precision 0.999/0.724 | recall 0.999/0.879 | f1 0.999/0.781\n",
      "[iteration 09000]accuracy 1.000/0.993 | precision 1.000/0.719 | recall 0.998/0.883 | f1 0.999/0.779\n",
      "[iteration 10000]accuracy 1.000/0.993 | precision 1.000/0.719 | recall 0.998/0.883 | f1 0.999/0.779\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 0\n",
    "for i in range(args.n_iterations):\n",
    "    x, y = next(loader)\n",
    "    ce = loss.CrossEntropyLoss()(c(x), y)\n",
    "    optimizer.zero_grad()\n",
    "    ce.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i + 1) % args.log_every == 0:\n",
    "        train_scores = global_scores(c, train_loader)\n",
    "        test_scores = global_scores(c, test_loader)\n",
    "\n",
    "        prefix = '0' * (len(str(args.n_iterations)) - len(str(i + 1)))\n",
    "        print('[iteration %s%d]' % (prefix, i + 1) + \\\n",
    "              ' | '.join('%s %0.3f/%0.3f' % (key, value, test_scores[key]) for key, value in train_scores.items()))\n",
    "\n",
    "        for key, value in train_scores.items():\n",
    "            writer.add_scalar('train-' + key, value, i)\n",
    "\n",
    "        for key, value in test_scores.items():\n",
    "            writer.add_scalar('test-' + key, value, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     49498\n",
      "          1       1.00      1.00      1.00       502\n",
      "\n",
      "avg / total       1.00      1.00      1.00     50000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00      9943\n",
      "          1       0.44      0.77      0.56        57\n",
      "\n",
      "avg / total       1.00      0.99      0.99     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(my.global_scores(c, train_loader, metrics.classification_report))\n",
    "print(my.global_scores(c, test_loader, metrics.classification_report))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
