{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import copy\n",
    "from threading import Condition, Thread\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.loss import CrossEntropyLoss, MSELoss\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_TRAIN, N_TEST = 0, 0\n",
    "train_data, train_labels, test_data, test_labels = my.unbalanced_cifar10(N_TRAIN, N_TEST, p=[])\n",
    "\n",
    "train_data_np, train_labels_np, test_data_np, test_labels_np = \\\n",
    "    train_data, train_labels, test_data, test_labels\n",
    "    \n",
    "train_data = th.from_numpy(train_data).float()\n",
    "train_labels = th.from_numpy(train_labels).long()\n",
    "test_data = th.from_numpy(test_data).float()\n",
    "test_labels = th.from_numpy(test_labels).long()\n",
    "\n",
    "cuda = True\n",
    "if cuda:\n",
    "    th.cuda.set_device(2)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(TensorDataset(train_data, train_labels), BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_data, test_labels), BATCH_SIZE)\n",
    "\n",
    "N_FEATURES = train_data.size()[1]\n",
    "N_CLASSES = int(train_labels.max() - train_labels.min() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, 3, 2, 1)\n",
    "        self.linear = nn.Linear(8, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dim() != 4:\n",
    "            x = x.view(-1, 3, 32, 32)\n",
    "        x = F.tanh(self.conv1(x))\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        x = F.avg_pool2d(x, 8)\n",
    "        x = self.linear(x.view(-1, 8))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# c = nn.Linear(N_FEATURES, N_CLASSES)\n",
    "# c = my.MLP((N_FEATURES,) + (64,) * 3 + (N_CLASSES,), F.relu)\n",
    "# c = CNN(N_CLASSES)\n",
    "# if cuda:\n",
    "#     c.cuda()\n",
    "# optim = Adam(c.parameters(), lr=0.001)\n",
    "# EPOCHS = 10\n",
    "# for i in range(EPOCHS):\n",
    "#     for j, (X, y) in enumerate(train_loader):\n",
    "#         if cuda:\n",
    "#             X, y = X.cuda(), y.cuda()\n",
    "#         X, y = Variable(X), Variable(y)\n",
    "#         loss = CrossEntropyLoss()(c(X), y)\n",
    "#         optim.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "#     accuracy = my.global_stats(c, test_loader, my.accuracy)\n",
    "#     print('[epoch %d]cross-entropy loss: %f, accuracy: %f' % ((i + 1), float(loss), float(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nd_stats = [my.accuracy] + [my.nd_curry(stat, N_CLASSES) for stat in (my.nd_precision, my.nd_recall, my.nd_f_beta)]\n",
    "# accuracy, precision, recall, f1 = my.global_stats(c, test_loader, nd_stats)\n",
    "# 'accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy, precision, recall, f1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "Let $c$ be a classifier and $D=\\{(X_1, y_1),...,(X_N, y_N)\\}$ be the set of training data. In order to minimize $L(c, D)$, where $L$ is a non-decomposable loss function, we introduce $L_\\theta$, a parameterized approximation of $L(c, D)$, and update $c$ as follows:\n",
    "\n",
    "1. Compute $\\delta = L(c, D)-L(\\bar{c},D)$, where $\\bar{c}$ is obtained by stochastically perturbing the parameters of $c$\n",
    "\n",
    "2. Randomly sample $K$ subsets, $D_1, ..., D_K$, of $D$ (these subsets may vary in cardinality)\n",
    "\n",
    "3. Minimize $(\\delta - \\frac1K \\sum_{i = 1}^K \\delta_i)^2$ with respect to $\\theta$, where $\\delta_i = L_\\theta(c, D_i) - L_\\theta(\\bar{c}, D_i)$\n",
    "\n",
    "4. Repeat 1, 2, and 3 several times until $L_\\theta$ becomes a satisfactory approximation of $L$ near $c$\n",
    "\n",
    "5. Randomly sample $K'$ subsets, $D_1, ..., D_K'$, of $D$ and let $c \\leftarrow c - \\alpha \\sum_{i = 1}^K \\frac{\\partial L_\\theta}{\\partial c} (c, D_i)$, where $\\alpha$ is a positive learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = lambda c, loader: my.global_stats(c, loader, my.nd_curry(my.nd_f_beta, N_CLASSES))\n",
    "\n",
    "def forward(classifier, pair):\n",
    "    x, y = pair\n",
    "    y = my.onehot(y, N_CLASSES)\n",
    "    y_bar = F.softmax(classifier(x), 1)\n",
    "    return th.cat((y, y_bar), 1).view(1, -1)\n",
    "\n",
    "def sample(sample_size, batch_size):\n",
    "    # TODO use DataLoader?\n",
    "    samples = [my.sample_subset(train_data_np, train_labels_np, sample_size) for k in range(batch_size)]\n",
    "    if cuda:\n",
    "        samples = [(x.cuda(), y.cuda()) for (x, y) in samples]\n",
    "    return [(Variable(x), Variable(y)) for (x, y) in samples]\n",
    "\n",
    "state_dict_cpu2gpu = lambda state_dict: {key : value.cuda() for key, value in state_dict.items()}\n",
    "state_dict_gpu2cpu = lambda state_dict: {key : value.cpu() for key, value in state_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy: 0.100000, precision: 0.010000, recall: 0.100000, f1: 0.018180'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_data, train_labels), 1024, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_data, test_labels), 1024)\n",
    "\n",
    "th.random.manual_seed(1)\n",
    "th.cuda.manual_seed_all(1)\n",
    "\n",
    "# c = nn.Linear(N_FEATURES, N_CLASSES)\n",
    "# c = my.MLP((N_FEATURES,) + (512,) * 1 + (N_CLASSES,), F.relu)\n",
    "c = CNN(N_CLASSES)\n",
    "critic = my.RN(SAMPLE_SIZE, 2 * N_CLASSES, (2048, 1024, 1024) + (1,), F.relu)\n",
    "\n",
    "if cuda:\n",
    "    c.cuda()\n",
    "    critic.cuda()\n",
    "\n",
    "# c_optim = SGD(c.parameters(), 0.1, momentum=0.5)\n",
    "# critic_optim = SGD(critic.parameters(), 0.1, momentum=0.5)\n",
    "c_optim = Adam(c.parameters(), 1e-3)\n",
    "critic_optim = Adam(critic.parameters(), 1e-3)\n",
    "\n",
    "nd_stats = [my.accuracy] + [my.nd_curry(stat, N_CLASSES) for stat in (my.nd_precision, my.nd_recall, my.nd_f_beta)]\n",
    "accuracy, precision, recall, f1 = my.global_stats(c, test_loader, nd_stats)\n",
    "'accuracy: %f, precision: %f, recall: %f, f1: %f' % tuple(map(float, (accuracy, precision, recall, f1)))\n",
    "# TODO unusual precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 1]f1: 0.042485\n",
      "[iteration 2]f1: 0.041974\n",
      "[iteration 3]f1: 0.059990\n",
      "[iteration 4]f1: 0.061109\n",
      "[iteration 5]f1: 0.077854\n",
      "[iteration 6]f1: 0.084579\n",
      "[iteration 7]f1: 0.104773\n"
     ]
    }
   ],
   "source": [
    "std = 1e-1\n",
    "tau = 1e-2\n",
    "\n",
    "N_ITERATIONS = 100\n",
    "N_PERTURBATIONS = 50\n",
    "CRITIC_ITERATIONS = 10\n",
    "ACTOR_ITERATIONS = 5\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "hist = []\n",
    "for i in range(N_ITERATIONS):\n",
    "    hist.append({})\n",
    "    hist[-1]['c_state_dict'] = copy.deepcopy(state_dict_gpu2cpu(c.state_dict()))\n",
    "    \n",
    "    c.eval()\n",
    "    critic.train()\n",
    "    L_c = L(c, test_loader)\n",
    "#     L_c = L(c, train_loader)\n",
    "    c_bar_list, t_list = [], []\n",
    "    f1_list = []\n",
    "    for j in range(N_PERTURBATIONS):\n",
    "        c_bar_list.append(my.perturb(c, std))\n",
    "        L_bar = L(c_bar_list[-1], test_loader)\n",
    "#         L_bar = L(c_bar_list[-1], train_loader)\n",
    "        f1_list.append(float(L_bar))\n",
    "        t = L_c - L_bar\n",
    "        t_list.append(t[0])\n",
    "    hist[-1]['f1_list'] = f1_list\n",
    "    w_list = [th.exp(-t ** 2 / tau) for t in t_list] # TODO proposal distribution\n",
    "    z = sum(w_list)\n",
    "    w_list = [(w / z).detach() for w in w_list]\n",
    "    hist[-1]['w_list'] = w_list\n",
    "    \n",
    "    s = sample(SAMPLE_SIZE, BATCH_SIZE)\n",
    "    hist[-1]['s'] = s\n",
    "    y = th.cat([forward(c, x) for x in s], 0).detach()\n",
    "    for j in range(CRITIC_ITERATIONS):\n",
    "        for c_bar, t, w in zip(c_bar_list, t_list, w_list):\n",
    "            y_bar = th.cat([forward(c_bar, x) for x in s], 0).detach()\n",
    "            delta = th.mean(critic(y) - critic(y_bar), 0)\n",
    "            mse = w * MSELoss()(delta, t)\n",
    "            critic_optim.zero_grad()\n",
    "            mse.backward()\n",
    "            critic_optim.step()\n",
    "    hist[-1]['critic_state_dict'] = copy.deepcopy(state_dict_gpu2cpu(critic.state_dict()))\n",
    "\n",
    "    c.train()\n",
    "    critic.eval()\n",
    "    c_parameters = copy.deepcopy(tuple(c.parameters()))\n",
    "    for j in range(ACTOR_ITERATIONS):\n",
    "        y = th.cat([forward(c, x) for x in s], 0)\n",
    "        objective = -th.mean(critic(y)) # TODO critic-awareness\n",
    "        c_optim.zero_grad()\n",
    "        objective.backward()\n",
    "        c_optim.step()\n",
    "        if any(float(th.max(th.abs(p - q))) > std for p, q in zip(c_parameters, c.parameters())):\n",
    "            break\n",
    "\n",
    "    if (i + 1) % 1 == 0:\n",
    "        f1 = my.global_stats(c, test_loader, my.nd_curry(my.nd_f_beta, N_CLASSES))\n",
    "        hist[-1]['f1'] = float(f1)\n",
    "        print('[iteration %d]f1: %f' % (i + 1, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# h = hist[-1]\n",
    "# c, c_bar = CNN(N_CLASSES), CNN(N_CLASSES)\n",
    "# c.load_state_dict(h['c_state_dict'])\n",
    "# c_bar.load_state_dict(h['c_state_dict'])\n",
    "# critic = my.RN(SAMPLE_SIZE, 2 * N_CLASSES, (1024,) * 3 + (1,), F.relu)\n",
    "# critic.load_state_dict(h['critic_state_dict'])\n",
    "# if cuda:\n",
    "#     c.cuda()\n",
    "#     c_bar.cuda()\n",
    "#     critic.cuda()\n",
    "\n",
    "# # TODO gradient in parameter space/simplex\n",
    "# y = th.cat([forward(c, x) for x in h['s']], 0)\n",
    "# objective = -th.mean(critic(y))\n",
    "# objective.backward()\n",
    "\n",
    "# alpha = np.linspace(0, 1e1)\n",
    "# f1_list = []\n",
    "# for a in alpha:\n",
    "#     for p, p_bar in zip(c.parameters(), c_bar.parameters()):\n",
    "#         p_bar.data = p.data - float(a) * p.grad.data\n",
    "#     f1_list.append(float(L(c_bar, test_loader)))\n",
    "\n",
    "# pl.plot(alpha, f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_list = [np.array(list(map(float, h['f1_list']))) for h in hist]\n",
    "min_list = tuple(map(np.min, f1_list))\n",
    "max_list = tuple(map(np.max, f1_list))\n",
    "std_list = tuple(map(np.std, f1_list))\n",
    "pl.plot(range(len(f1_list)), min_list, label='min sample')\n",
    "pl.plot(range(len(f1_list)), max_list, label='max sample')\n",
    "pl.plot(range(len(hist)), [h['f1'] for h in hist], label='classifier')\n",
    "pl.title('f1')\n",
    "pl.legend(framealpha=0)\n",
    "pl.figure()\n",
    "pl.title('std')\n",
    "pl.plot(range(len(f1_list)), std_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy_list = []\n",
    "for h in hist:\n",
    "    w_array = np.array(list(map(float, h['w_list'])))\n",
    "    entropy_list.append(-np.sum(w_array * np.log(w_array)))\n",
    "n = len(hist)\n",
    "pl.plot(range(n), entropy_list, label='importance distribution')\n",
    "pl.plot(range(n), -np.ones(n) * np.log(1 / N_PERTURBATIONS), label='uniform distribution')\n",
    "pl.title('entropy')\n",
    "pl.legend(framealpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_list = [np.array(list(map(float, h['w_list']))) for h in hist]\n",
    "min_list = tuple(map(np.min, w_list))\n",
    "max_list = tuple(map(np.max, w_list))\n",
    "std_list = tuple(map(np.std, w_list))\n",
    "pl.plot(range(len(w_list)), min_list, label='min')\n",
    "pl.plot(range(len(w_list)), max_list, label='max')\n",
    "pl.title('importance weight')\n",
    "pl.legend(framealpha=0)\n",
    "pl.figure()\n",
    "pl.title('importance weight')\n",
    "pl.plot(range(len(w_list)), std_list, label='std')\n",
    "pl.legend(framealpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# critic for mnist\n",
    "\n",
    "# N_ITERATIONS = 25\n",
    "# BATCH_SIZE = 8\n",
    "\n",
    "# critic.load_state_dict(th.load('mnist_critic'))\n",
    "# for i in range(N_ITERATIONS):\n",
    "#     s = sample(SAMPLE_SIZE, BATCH_SIZE)\n",
    "#     y = th.cat([forward(c, x) for x in s], 0)\n",
    "#     objective = -th.mean(critic(y))\n",
    "#     c_optim.zero_grad()\n",
    "#     objective.backward()\n",
    "#     c_optim.step()\n",
    "        \n",
    "#     if (i + 1) % 1 == 0:\n",
    "#         f1 = my.global_stats(c, test_loader, my.nd_curry(my.nd_f_beta, N_CLASSES))\n",
    "#         print('[iteration %d]f1: %f' % (i + 1, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO parallel sampling\n",
    "\n",
    "# gpus = [0, 1, 2, 3]\n",
    "# train_data_dict = {gpu: train_data.cuda(gpu) for gpu in gpus}\n",
    "# train_labels_dict = {gpu: train_labels.cuda(gpu) for gpu in gpus}\n",
    "\n",
    "# def L_batch(c, std, n):\n",
    "#     c_device = next(c.parameters()).get_device()\n",
    "#     c = deepcopy(c).cpu()\n",
    "#     c.train(False)\n",
    "#     c_dict = {gpu: deepcopy(c).cuda(gpu) for gpu in gpus}\n",
    "#     results = []\n",
    "#     available_gpus = [gpu for gpu in gpus]\n",
    "#     condition = Condition()\n",
    "    \n",
    "#     def target(gpu):\n",
    "#         c, train_data, train_labels = c_dict[gpu], train_data_dict[gpu], train_labels_dict[gpu]\n",
    "#         c_bar = my.perturb(c, std)\n",
    "        \n",
    "#         delta = L(c, train_data, train_labels) - L(c_bar, train_data, train_labels) # TODO\n",
    "#         results.append((c_bar.cuda(c_device), delta.cuda(c_device).detach()))\n",
    "#         with condition:\n",
    "#             available_gpus.append(gpu)\n",
    "#             condition.notify_all()\n",
    "    \n",
    "#     threads = []\n",
    "#     with condition:\n",
    "#         for i in range(n):\n",
    "#             if not available_gpus:\n",
    "#                 condition.wait()\n",
    "#             gpu = available_gpus.pop()\n",
    "#             threads.append(Thread(target=target, args=(gpu,)))\n",
    "#             threads[-1].start()\n",
    "#     for t in threads:\n",
    "#         t.join()\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO parallel sampling\n",
    "\n",
    "# STD = 0.05\n",
    "# OUTER = 5000\n",
    "# INNER_ACTOR = 5\n",
    "# INNER_CRITIC = 20\n",
    "\n",
    "# mse_list = []\n",
    "# for i in range(OUTER):\n",
    "#     targets = L_batch(c, STD, INNER_CRITIC)\n",
    "#     for j in range(INNER_CRITIC):\n",
    "#         c_bar, delta = targets[j]\n",
    "#         samples = sample()\n",
    "#         y = th.cat(tuple(map(lambda x: forward(c, x), samples)), 0).detach()\n",
    "#         y_bar = th.cat(tuple(map(lambda x: forward(c_bar, x), samples)), 0).detach()\n",
    "        \n",
    "#         mse = 1\n",
    "#         while float(mse) > 1e-3:\n",
    "#             delta_ = th.mean(critic(y) - critic(y_bar), 0)\n",
    "#             mse = MSELoss()(delta_, delta)\n",
    "#             mse_list.append(mse)\n",
    "#             critic_optim.zero_grad()\n",
    "#             mse.backward()\n",
    "#             critic_optim.step()\n",
    "\n",
    "#     c_parameters = deepcopy(tuple(c.parameters()))\n",
    "#     for j in range(INNER_ACTOR):\n",
    "#         samples = sample()\n",
    "#         y = th.cat(tuple(map(lambda x: forward(c, x), samples)), 0)\n",
    "#         objective = -th.mean(critic(y))\n",
    "#         c_optim.zero_grad()\n",
    "#         objective.backward()\n",
    "#         c_optim.step()\n",
    "#         if any(float(th.max(th.abs(p - q))) > STD for p, q in zip(c_parameters, c.parameters())):\n",
    "#             break\n",
    "\n",
    "#     if (i + 1) % 1 == 0:\n",
    "#         y_bar = my.predict(c, test_data)\n",
    "#         f1 = my.nd_f_beta(y_bar, test_labels, N_CLASSES)\n",
    "#         print('[iteration %d]mse: %f, objective: %f, f1: %f' % ((i + 1), float(mse), float(objective), float(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO F_beta implementation\n",
    "\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# y_bar = my.predict(c, test_data).data.cpu().numpy()\n",
    "# precision_score(test_labels_np, y_bar, average='macro'), \\\n",
    "# recall_score(test_labels_np, y_bar, average='macro'), \\\n",
    "# f1_score(test_labels_np, y_bar, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
