{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import collections\n",
    "import time\n",
    "import sklearn.metrics as metrics\n",
    "import tensorboardX as tb\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "import data\n",
    "import es\n",
    "import my\n",
    "import lenet\n",
    "import resnet\n",
    "import rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.actor = 'linear'\n",
    "# args.actor = 'lenet'\n",
    "# args.actor = 'resnet'\n",
    "args.alpha = 0.5\n",
    "args.average = 'binary'\n",
    "args.batch_size_actor = 100\n",
    "args.batch_size_criticx = 1\n",
    "args.batch_size_criticy = 1\n",
    "args.beta = 2\n",
    "args.ckpt_every = 0\n",
    "# args.dataset = 'mnist'\n",
    "args.dataset = 'cifar10'\n",
    "args.gpu = 0\n",
    "args.guided_es = True\n",
    "args.iw = 'none'\n",
    "# args.iw = 'sqrt'\n",
    "# args.iw = 'linear'\n",
    "# args.iw = 'quadratic'\n",
    "args.k = 10\n",
    "# args.labelling = ''\n",
    "args.labelling = '91'\n",
    "args.n_iterations = 50\n",
    "args.n_iterations_actor = 25\n",
    "args.n_iterations_critic = 25\n",
    "args.n_perturbations = 50\n",
    "args.report_every = 10\n",
    "args.resume = 0\n",
    "args.std = 1\n",
    "args.sample_size_critic = 1\n",
    "args.tau = 0.1\n",
    "args.tb = False\n",
    "args.verbose = -1\n",
    "\n",
    "'''\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--average', type=str, default=None)\n",
    "parser.add_argument('--batch-size-actor', type=int, default=None)\n",
    "parser.add_argument('--batch-size-critic', type=int, default=None)\n",
    "parser.add_argument('--ckpt-every', type=int, default=None)\n",
    "parser.add_argument('--dataset', type=str, default=None)\n",
    "parser.add_argument('--gpu', type=int, default=None)\n",
    "parser.add_argument('--iw', type=str, default=None)\n",
    "parser.add_argument('--report-every', type=int, default=None)\n",
    "parser.add_argument('--actor', type=str, default=None)\n",
    "parser.add_argument('--labelling', type=str, default=None)\n",
    "parser.add_argument('--n-iterations', type=int, default=None)\n",
    "parser.add_argument('--n-iterations-critic', type=int, default=None)\n",
    "parser.add_argument('--n-perturbations', type=int, default=None)\n",
    "parser.add_argument('--resume', type=int, default=None)\n",
    "parser.add_argument('--sample-size-critic', type=int, default=None)\n",
    "parser.add_argument('--std', type=float, default=None)\n",
    "parser.add_argument('--tau', type=float, default=None)\n",
    "parser.add_argument('--verbose', type=int, default=-1)\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "\n",
    "keys = sorted(vars(args).keys())\n",
    "excluded = ('ckpt_every', 'gpu', 'report_every', 'n_iterations', 'resume', 'tb', 'verbose')\n",
    "experiment_id = 'parameter-' + '-'.join('%s-%s' % (key, str(getattr(args, key))) for key in keys if key not in excluded)\n",
    "if args.tb:\n",
    "    writer = tb.SummaryWriter('runs/' + experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.gpu < 0:\n",
    "    cuda = False\n",
    "    new_tensor = th.FloatTensor\n",
    "else:\n",
    "    cuda = True\n",
    "    new_tensor = th.cuda.FloatTensor\n",
    "    th.cuda.set_device(args.gpu)\n",
    "\n",
    "labelling = {} if args.labelling == '' else {(0, 9) : 0, (9, 10) : 1}\n",
    "rbg = args.actor in ('lenet', 'resnet')\n",
    "train_x, train_y, test_x, test_y = getattr(data, 'load_%s' % args.dataset)(labelling, rbg, torch=True)\n",
    "\n",
    "train_set = utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = utils.data.DataLoader(train_set, 4096, drop_last=False)\n",
    "test_set = utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = utils.data.DataLoader(test_set, 4096, drop_last=False)\n",
    "\n",
    "loader = data.BalancedDataLoader(train_x, train_y, args.batch_size_actor, cuda)\n",
    "\n",
    "n_classes = int(train_y.max() - train_y.min() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(tensor, batch_sizex, batch_sizey):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : (x, y, z)\n",
    "    \"\"\"\n",
    "    shapex, shapey, shapez = tensor.shape\n",
    "    nx, ny = int(shapex / batch_sizex), int(shapey / batch_sizey)\n",
    "    x_list = th.chunk(tensor, nx, 0)\n",
    "    return sum([[y.view(-1, shapez) for y in th.chunk(x, ny, 1)] for x in x_list], [])\n",
    "\n",
    "def forward(actor, batch_list, yz=True, L=True):\n",
    "    x_tuple, y_tuple = zip(*batch_list)\n",
    "    x_tensor, y_tensor = th.cat(x_tuple), th.cat(y_tuple)\n",
    "    z_tensor = actor(x_tensor)\n",
    "    ret = []\n",
    "    if yz:\n",
    "        ret.append(th.cat([my.onehot(y_tensor, n_classes), F.softmax(z_tensor, 1)], 1).view(len(batch_list), -1))\n",
    "    if L:\n",
    "        z_list = th.chunk(z_tensor, len(batch_list))\n",
    "        ret.append(new_tensor([L_batch(y, z) for y, z in zip(y_tuple, z_list)]).unsqueeze(1))\n",
    "    return ret\n",
    "    \n",
    "def L_batch(y, y_bar):\n",
    "    y_bar = th.max(y_bar, 1)[1]\n",
    "    return metrics.f1_score(y, y_bar, average=args.average)\n",
    "\n",
    "iw = {\n",
    "    'none' : lambda x: th.zeros_like(x),\n",
    "    'quadratic' : lambda x: x * x,\n",
    "}[args.iw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ckpt(actor, critic, actor_optim, critic_optim, i):\n",
    "    th.save(actor.state_dict(), 'ckpt/%s-actor-%d' % (experiment_id, i + 1))\n",
    "    th.save(critic.state_dict(), 'ckpt/%s-critic-%d' % (experiment_id, i + 1))\n",
    "    th.save(actor_optim.state_dict(), 'ckpt/%s-actor_optim-%d' % (experiment_id, i + 1))\n",
    "    th.save(critic_optim.state_dict(), 'ckpt/%s-critic_optim-%d' % (experiment_id, i + 1))\n",
    "\n",
    "def global_scores(c, loader):\n",
    "    key_list = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    score_list = [\n",
    "        metrics.accuracy_score,\n",
    "        lambda y, y_bar: metrics.precision_recall_fscore_support(y, y_bar, average=args.average)\n",
    "    ]\n",
    "    accuracy, (precision, recall, f1, _) = my.global_scores(c, loader, score_list)\n",
    "    return collections.OrderedDict({\n",
    "        'accuracy'  : accuracy,\n",
    "        'precision' : precision,\n",
    "        'recall'    : recall,\n",
    "        'f1'        : f1,\n",
    "    })\n",
    "\n",
    "def log_stats(tensor, tag, i):\n",
    "    writer.add_scalar('th.min(%s)' % tag, th.min(tensor), i + 1)\n",
    "    writer.add_scalar('th.max(%s)' % tag, th.max(tensor), i + 1)\n",
    "    writer.add_scalar('th.mean(%s)' % tag, th.mean(tensor), i + 1)\n",
    "    \n",
    "def report(actor, i):\n",
    "    train_scores = global_scores(actor, train_loader)\n",
    "    test_scores = global_scores(actor, test_loader)\n",
    "\n",
    "    prefix = '0' * (len(str(args.n_iterations)) - len(str(i + 1)))\n",
    "    print('[iteration %s%d]' % (prefix, i + 1) + \\\n",
    "          ' | '.join('%s %0.3f/%0.3f' % (key, value, test_scores[key]) for key, value in train_scores.items()))\n",
    "\n",
    "    if args.tb:\n",
    "        for key, value in train_scores.items():\n",
    "            writer.add_scalar('train-' + key, value, i + 1)\n",
    "        for key, value in test_scores.items():\n",
    "            writer.add_scalar('test-' + key, value, i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "es = imp.reload(es)\n",
    "data = imp.reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 00]accuracy 0.454/0.457 | precision 0.267/0.291 | recall 0.054/0.058 | f1 0.089/0.097\n"
     ]
    }
   ],
   "source": [
    "th.random.manual_seed(1)\n",
    "if cuda:\n",
    "    th.cuda.manual_seed_all(1)\n",
    "\n",
    "n_channels = 1 if args.dataset == 'mnist' else 3\n",
    "size = 28 if args.dataset == 'mnist' else 32\n",
    "actor = {\n",
    "    'linear' : nn.Linear(n_channels * size ** 2, n_classes),\n",
    "    'lenet'  : lenet.LeNet(3, n_classes, size),\n",
    "    'resnet' : resnet.ResNet(depth=18, n_classes=n_classes),\n",
    "}[args.actor]\n",
    "unary = [2 * n_classes, 64]\n",
    "binary = [2 * unary[-1], 64]\n",
    "terminal = [64, 64] + [1]\n",
    "critic = rn.RN(args.batch_size_actor, 2 * n_classes, unary, binary, terminal, F.relu, triu=True)\n",
    "\n",
    "if cuda:\n",
    "    actor.cuda()\n",
    "    critic.cuda()\n",
    "\n",
    "if args.guided_es:\n",
    "    guided_es = es.GuidedES(args.std, args.alpha, my.n_parameters(actor), args.k)\n",
    "    \n",
    "actor_optim = optim.Adam(actor.parameters(), amsgrad=True)\n",
    "critic_optim = optim.Adam(critic.parameters(), amsgrad=True)\n",
    "\n",
    "if args.resume > 0:\n",
    "    c.load_state_dict(th.load('ckpt/%s-actor-%d' % (experiment_id, args.resume)))\n",
    "    critic.load_state_dict(th.load('ckpt/%s-critic-%d' % (experiment_id, args.resume)))\n",
    "    c_optim.load_state_dict(th.load('ckpt/%s-actor_optim-%d' % (experiment_id, args.resume)))\n",
    "    critic_optim.load_state_dict(th.load('ckpt/%s-critic_optim-%d' % (experiment_id, args.resume)))\n",
    "\n",
    "report(actor, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(args.resume, args.resume + args.n_iterations):\n",
    "    batch_list = [next(loader) for j in range(args.sample_size_critic)]\n",
    "    \n",
    "    my.set_requires_grad(actor, False)\n",
    "    yz, L = forward(actor, batch_list)\n",
    "\n",
    "    actor_bar = copy.deepcopy(actor)\n",
    "    my.set_requires_grad(actor_bar, False)\n",
    "    yzbar_list, Lbar_list, delta_list = [], [], []\n",
    "    for j in range(args.n_perturbations):\n",
    "        my.copy(actor, actor_bar)\n",
    "        if args.guided_es:\n",
    "            guided_es.perturb(actor_bar)\n",
    "        else:\n",
    "            my.perturb(actor_bar, args.std)\n",
    "        yz_bar, L_bar = forward(actor_bar, batch_list)\n",
    "        yzbar_list.append(yz_bar)\n",
    "        Lbar_list.append(L_bar)\n",
    "        delta_list.append(L - L_bar)\n",
    "    if args.tb:\n",
    "        log_stats(th.cat(Lbar_list, 1), 'L_bar', i)\n",
    "        \n",
    "    yzbar_tensor = th.cat([yz_bar.unsqueeze(1) for yz_bar in yzbar_list], 1)\n",
    "    delta_tensor = th.cat(delta_list, 1)\n",
    "    weight_tensor = F.softmax(iw(delta_tensor), 1)\n",
    "    entropy = th.sum(weight_tensor * th.log(weight_tensor)) / args.sample_size_critic\n",
    "    if args.tb:\n",
    "        writer.add_scalar('entropy', entropy, i + 1)\n",
    "    \n",
    "    delta_tensor, weight_tensor = delta_tensor.unsqueeze(2), weight_tensor.unsqueeze(2)\n",
    "    lambda_batch = lambda tensor: batch(tensor, args.batch_size_criticx, args.batch_size_criticy)\n",
    "    yzbar_list, delta_list, weight_list = list(map(lambda_batch, [yzbar_tensor, delta_tensor, weight_tensor]))\n",
    "    \n",
    "    my.set_requires_grad(critic, True)\n",
    "    for j in range(args.n_iterations_critic):\n",
    "        for yz_bar, delta, weight in zip(yzbar_list, delta_list, weight_list):\n",
    "            mse = th.sum(weight * (delta - (critic(yz) - critic(yz_bar))) ** 2)\n",
    "            critic_optim.zero_grad()\n",
    "            mse.backward()\n",
    "            critic_optim.step()\n",
    "        if args.tb:\n",
    "            writer.add_scalar('mse', mse, i * args.n_iterations_critic + j + 1)\n",
    "\n",
    "    my.set_requires_grad(actor, True)\n",
    "    my.set_requires_grad(critic, False)\n",
    "    for j in range(args.n_iterations_actor):\n",
    "        yz, L = forward(actor, batch_list)\n",
    "        if args.tb:\n",
    "            log_stats(L, 'L', i * args.n_iterations_actor + j)\n",
    "        \n",
    "        objective = -critic(yz)\n",
    "        actor_optim.zero_grad()\n",
    "        objective.backward()\n",
    "        actor_optim.step()\n",
    "        \n",
    "    if args.report_every > 0 and (i + 1) % args.report_every == 0:\n",
    "        report(actor, i)\n",
    "\n",
    "    if args.ckpt_every > 0 and (i + 1) % args.ckpt_every == 0:\n",
    "        ckpt(actor, critic, actor_optim, critic_optim, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
