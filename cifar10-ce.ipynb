{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import sklearn.metrics as metrics\n",
    "import tensorboardX as tb\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.modules.loss as loss\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "import data\n",
    "import my\n",
    "import lenet\n",
    "import resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.batch_size = 50\n",
    "args.gpu = 0\n",
    "args.n_iterations = 500000\n",
    "\n",
    "keys = sorted(vars(args).keys())\n",
    "excluded = ('gpu',)\n",
    "run_id = 'ce-' + '-'.join('%s-%s' % (key, str(getattr(args, key))) for key in keys if key not in excluded)\n",
    "writer = tb.SummaryWriter('runs/' + run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.gpu < 0:\n",
    "    cuda = False\n",
    "else:\n",
    "    cuda = True\n",
    "    th.cuda.set_device(args.gpu)\n",
    "\n",
    "train_x, train_y, test_x, test_y = data.load_cifar10(rbg=True, torch=True)\n",
    "\n",
    "train_set = utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = utils.data.DataLoader(train_set, 4096, drop_last=False)\n",
    "test_set = utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = utils.data.DataLoader(test_set, 4096, drop_last=False)\n",
    "\n",
    "def TrainLoader():\n",
    "    dataset = utils.data.TensorDataset(train_x, train_y)\n",
    "    new_loader = lambda: iter(utils.data.DataLoader(dataset, args.batch_size, shuffle=True))\n",
    "    contextualize = lambda x, y: (x.cuda(), y.cuda()) if cuda else (x, y)\n",
    "    while True:\n",
    "        try:\n",
    "            yield contextualize(*next(loader))\n",
    "        except:\n",
    "            loader = new_loader()\n",
    "            yield contextualize(*next(loader))\n",
    "\n",
    "loader = TrainLoader()\n",
    "\n",
    "n_classes = int(train_y.max() - train_y.min() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_scores(c, loader):\n",
    "    keys = ('accuracy', 'precision', 'recall', 'f1')\n",
    "    scores = (\n",
    "        metrics.accuracy_score,\n",
    "        lambda y, y_bar: metrics.precision_score(y, y_bar, average='micro'),\n",
    "        lambda y, y_bar: metrics.recall_score(y, y_bar, average='micro'),\n",
    "        lambda y, y_bar: metrics.f1_score(y, y_bar, average='micro'),\n",
    "    )\n",
    "    values = [value.item() for value in my.global_scores(c, loader, scores)]\n",
    "    return collections.OrderedDict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = lenet.LeNet(3, n_classes)\n",
    "c = resnet.ResNet(18, n_classes)\n",
    "\n",
    "if cuda:\n",
    "    c.cuda()\n",
    "    \n",
    "# optimizer = optim.SGD(c.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer = optim.Adam(c.parameters())\n",
    "\n",
    "for key, value in global_scores(c, test_loader).items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_iterations = 0\n",
    "for i in range(args.n_iterations):\n",
    "    x, y = next(loader)\n",
    "    ce = loss.CrossEntropyLoss()(c(x), y)\n",
    "    optimizer.zero_grad()\n",
    "    ce.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_scores = global_scores(c, train_loader)\n",
    "    test_scores = global_scores(c, test_loader)\n",
    "\n",
    "    prefix = '0' * (len(str(args.n_iterations)) - len(str(i + 1)))\n",
    "    print('[iteration %s%d]' % (prefix, i + 1) + \\\n",
    "          ' | '.join('%s %0.3f/%0.3f' % (key, value, test_scores[key]) for key, value in train_scores.items()))\n",
    "\n",
    "    for key, value in train_scores.items():\n",
    "        writer.add_scalar('train-' + key, value, i)\n",
    "\n",
    "    for key, value in test_scores.items():\n",
    "        writer.add_scalar('test-' + key, value, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pg in optimizer.param_groups:\n",
    "#     pg['lr'] *= 0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
